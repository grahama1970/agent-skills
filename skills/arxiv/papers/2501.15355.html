<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2501.15355] ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection</title><meta property="og:description" content="Recent studies have increasingly demonstrated that large language models (LLMs) possess significant theory of mind (ToM) capabilities, showing the potential for simulating the tracking of mental states in generative ag…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2501.15355">
<link rel="canonical" target="_blank" href="https://ar5iv.labs.arxiv.org/html/2501.15355">

<!--Generated on Wed Feb  5 15:41:45 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.4.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.4.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bo Yang *, Jiaxian Guo  , Yusuke Iwasawa, Yutaka Matsuo 
<br class="ltx_break">The University of Tokyo 
<br class="ltx_break"><span class="ltx_text ltx_font_typewriter">bo-yang@weblab.t.u-tokyo.ac.jp</span>
</span><span class="ltx_author_notes">Equal Contribution.</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p">Recent studies have increasingly demonstrated that large language models (LLMs) possess significant <span class="ltx_text ltx_font_bold">theory of mind (ToM)</span> capabilities, showing the potential for simulating the tracking of mental states in generative agents.
In this study, we propose a novel paradigm called <span class="ltx_text ltx_font_bold">ToM-agent</span>, designed to empower LLMs-based generative agents to simulate ToM in open-domain conversational interactions.
ToM-agent disentangles the confidence from mental states, facilitating the emulation of an agent’s perception of its counterpart’s mental states, such as <span class="ltx_text ltx_font_bold">beliefs, desires, and intentions (BDIs)</span>.
Using past conversation history and verbal reflections, ToM-Agent can dynamically adjust counterparts’ inferred BDIs, along with related confidence levels.
We further put forth a counterfactual intervention method that reflects on the gap between the predicted responses of counterparts and their real utterances, thereby enhancing the efficiency of reflection.
Leveraging empathetic and persuasion dialogue datasets, we assess the advantages of implementing the ToM-agent with downstream tasks, as well as its performance in both the <span class="ltx_text ltx_font_italic">first-order</span> and the <span class="ltx_text ltx_font_italic">second-order</span> ToM.
Our findings indicate that the ToM-agent can grasp the underlying reasons for their counterpart’s behaviors beyond mere semantic-emotional supporting or decision-making based on common sense, providing new insights for studying large-scale LLMs-based simulation of human social behaviors.
<span class="ltx_text ltx_font_italic">The codes of this project will be made publicly available after the paper acceptance.</span></p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2501.15355/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="312" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustrations of proposed ToM-agent with <span class="ltx_text ltx_font_bold">BDIs tracking paradigm</span> for LLMs-based generative agents aware <span class="ltx_text ltx_font_bold">theory of mind (ToM)</span> and <span class="ltx_text ltx_font_bold">counterfactual reflection</span>. As LLMs-based generative agents, two NPCs Bob and Alice are in conversational communication with each other. Bob generates his utterance based on the conversation history and his own <span class="ltx_text ltx_font_bold">beliefs, desires, and intentions (BDIs)</span>. Alice infers about Bob’s top-k BDI candidates with confidence accordingly and predicts Bob’s next-round response based on the conversation history and inferred BDIs. Then the counterfactual reflection is conducted based on the gap between the real response of Bob and the predicted response to make an updated plan, including add or delete manipulations for inferred top-k BDIs. Finally, Alice carries out the plan to update the inferred top-k BDIs of Bob along with the confidences accordingly.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Generative agents <cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, which are computational interactive agents with critical components such as memory, observation, planning, and reflection, have been proposed to simulate believable human behavior during conversational interactions by fusing with LLMs <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a href="#bib.bib31" title="" class="ltx_ref">2019</a>; Brown et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Ouyang et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>; OpenAI, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>; Touvron et al., <a href="#bib.bib44" title="" class="ltx_ref">2023</a>)</cite>. Nevertheless, the limitations of LLMs in generating extended, coherent dialogues are well-documented, particularly their proclivity for generating hallucinated or inconsistent content <cite class="ltx_cite ltx_citemacro_citep">(Rawte et al., <a href="#bib.bib33" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib52" title="" class="ltx_ref">2023</a>)</cite>. These shortcomings are especially problematic when the purpose of the conversation extends beyond simple information exchange to include emotive or persuasive elements, such as in scenarios of emotional support, sales, or persuasive communication <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a href="#bib.bib15" title="" class="ltx_ref">2023</a>; tse Huang et al., <a href="#bib.bib45" title="" class="ltx_ref">2023</a>; Remountakis et al., <a href="#bib.bib34" title="" class="ltx_ref">2023</a>)</cite>. Such situations necessitate not merely the exchange of factual information, but also the articulation of nuanced demands or emotional appeals <cite class="ltx_cite ltx_citemacro_citep">(Yakura, <a href="#bib.bib50" title="" class="ltx_ref">2023</a>)</cite>, which current LLMs architectures struggle to maintain across natural and prolonged conversational sequences <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al., <a href="#bib.bib53" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">Referring to psychology science, it is noticed that human normally do not only express their emotions or demands during interaction but also care about their own or counterpart’s mental status, such as <span class="ltx_text ltx_font_bold">beliefs, desires, and intentions (BDIs)</span> has been understood or satisfied during the communications <cite class="ltx_cite ltx_citemacro_citep">(Dvash &amp; Shamay-Tsoory, <a href="#bib.bib9" title="" class="ltx_ref">2014</a>; Grazzani et al., <a href="#bib.bib12" title="" class="ltx_ref">2018</a>; Rusch et al., <a href="#bib.bib35" title="" class="ltx_ref">2020</a>)</cite>.
<span class="ltx_text ltx_font_bold">Theory of mind (ToM)</span>, a cognitive skill that enables an individual to track the BDIs, emotions, and knowledge of others, plays a crucial role in effective communication, self-consciousness, empathetic emotional support, and decision-making when human beings interact with each other, as well as interactions between human beings and artificial intelligence (AI) <cite class="ltx_cite ltx_citemacro_citep">(Georgeff et al., <a href="#bib.bib11" title="" class="ltx_ref">1999</a>; Rabinowitz et al., <a href="#bib.bib30" title="" class="ltx_ref">2018</a>; Nguyen &amp; González, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>)</cite>.
To mimic human behaviors caused by BDIs, we propose equating generative agents with cognitive and emotional reasoning abilities with ToM capacity.</p>
</div>
<div id="S1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">With the recent striking progress in LLMs, several researchers have endeavored to evaluate the ToM reasoning capabilities of these models <cite class="ltx_cite ltx_citemacro_citep">(Sahu et al., <a href="#bib.bib37" title="" class="ltx_ref">2022</a>; Gandhi et al., <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Jamali et al., <a href="#bib.bib16" title="" class="ltx_ref">2023</a>; Street et al., <a href="#bib.bib42" title="" class="ltx_ref">2024</a>; Strachan et al., <a href="#bib.bib41" title="" class="ltx_ref">2024</a>)</cite>.
Studies have utilized both commercial models (such as OpenAI’s GPT series <cite class="ltx_cite ltx_citemacro_citep">(Brown et al., <a href="#bib.bib4" title="" class="ltx_ref">2020</a>; Ouyang et al., <a href="#bib.bib26" title="" class="ltx_ref">2022</a>; OpenAI, <a href="#bib.bib25" title="" class="ltx_ref">2023</a>)</cite> and Anthropic’s Claude series) and non-commercial models (such as Meta’s LLaMa series and Google’s PaLM series).
However, prior research in psychology accessing the ToM reasoning ability of LLMs was limited to psychology statical benchmarks-based evaluation: single-word completion <cite class="ltx_cite ltx_citemacro_citep">(Kosinski, <a href="#bib.bib18" title="" class="ltx_ref">2023</a>; Ullman, <a href="#bib.bib46" title="" class="ltx_ref">2023</a>)</cite>, or multiple-option completion <cite class="ltx_cite ltx_citemacro_citep">(Sap et al., <a href="#bib.bib38" title="" class="ltx_ref">2023</a>)</cite>, or pre-written stories based on specified psychological tasks, or story comprehension scenarios <cite class="ltx_cite ltx_citemacro_citep">(Moghaddam &amp; Honey, <a href="#bib.bib22" title="" class="ltx_ref">2023</a>; Sclar et al., <a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite>, or playing incomplete information game <cite class="ltx_cite ltx_citemacro_citep">(Guo et al., <a href="#bib.bib13" title="" class="ltx_ref">2023</a>)</cite>.
Moreover, some other works attempt to investigate the possibilities of LLMs-based ToM modeling in natural conversation scenarios but are confined to specific collaborative task-oriented conversational scenarios such as agents for education <cite class="ltx_cite ltx_citemacro_citep">(Saha et al., <a href="#bib.bib36" title="" class="ltx_ref">2023</a>)</cite>, stress-testing <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>, multi-agents for collaborative behaviors <cite class="ltx_cite ltx_citemacro_citep">(Bara et al., <a href="#bib.bib2" title="" class="ltx_ref">2021</a>; Li et al., <a href="#bib.bib19" title="" class="ltx_ref">2023</a>)</cite>.
Nevertheless, previous studies have been focused more on the study of belief and tend to interpret belief as a mechanism of belief tracking <cite class="ltx_cite ltx_citemacro_citep">(Sclar et al., <a href="#bib.bib39" title="" class="ltx_ref">2023</a>)</cite>, so that even in natural language conversations, often the content of the conversation is limited to specific tasks or specific topic scenarios <cite class="ltx_cite ltx_citemacro_citep">(Qiu et al., <a href="#bib.bib29" title="" class="ltx_ref">2023</a>; Zhang et al., <a href="#bib.bib51" title="" class="ltx_ref">2024</a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">In this study, we extend the ToM to open-domain conversational interaction scenarios by proposing generative agents leveraging a novel BDI tracking paradigm: <span class="ltx_text ltx_font_bold">ToM-agent</span>, which is illustrated in <span class="ltx_text ltx_font_bold">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>.
In contrast to existing models or paradigms that limit task-specific computable ToM models to binary conditions (true belief or false belief), the proposed <span class="ltx_text ltx_font_bold">ToM-Agent</span> can disentangle the <span class="ltx_text ltx_font_bold">belief</span> and <span class="ltx_text ltx_font_bold">confidence</span> based on psychological research.
This allows for the simulation of generative agents engaged in open-domain conversational interactions that consider varying degrees of confidence in different mental states, such as BDIs, knowledge, and more.
More specifically, unlike the previous works that limited the ToM to studying confidence about certain situations or facts, our work could extend the ToM study to a specific person’s BDIs, which are different from the commonsense knowledge with the highest priority in the generation process by LLMs.
Further, when one ToM-equipped agent engages in a whispered conversation with the counterpart agent, it can generate utterances based on its BDIs.
Additionally, it can infer the BDIs of the counterpart agent (first-order ToM), as well as the counterpart’s cognitive thinking about its own BDIs (second-order ToM).
With the processing of the conversation, agents will also update their confidence in the counterpart’s BDI according to dialogue history and the reflection of confidence on the observations.
Counterfactual reflection mechanisms are also introduced to enhance the reflection performance minding the gap between the predicted responses and the real observed responses.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">To evaluate the proposed paradigm, we conducted a simulation experiment between agents based on two downstream conversational tasks: empathetic dialogue and persuasion dialogue.
Further, ablation studies with different LLMs and prompting methods are conducted to confirm the performance of the ToM-agent.
The main contribution of this work could be concluded as follows:</p>
</div>
<div id="S1.p6" class="ltx_para ltx_noindent">
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To the best of our knowledge, our work is the first to apply ToM modeling to open-domain conversational interactions for generative agents by disentangling the belief and confidence, in contrast to previous studies that were confined to psychological narratives or specific task-oriented cooperative conversation scenarios.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">By leveraging the zero-shot power of LLMs, generative agents are allowed to autonomously produce utterances based on their BDIs and infer their counterparts’ BDIs based on the conversation context without necessitating training on collected dialogue corpus with annotation.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The counterfactual reflection method is introduced to reason about the discrepancy between the predicted response and the real response, thus indirectly reflecting on the gap between the real BDIs and the predicted BDIs, thereby enhancing the effectiveness of updating confidence on inferred BDIs.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In experiments conducted on downstream tasks related to conversational interactions, the effectiveness of the proposed ToM tracking paradigm has been confirmed, and we posit that the implications of our findings extend beyond the AI research community, potentially offering valuable insights into the field of psychology and other scientific areas.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>

<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Theory of mind (ToM)</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">”What is ToM?”</span> and <span class="ltx_text ltx_font_italic">”Why ToM is important for Artificial intelligence?”</span> are two questions we would like to stress at the very beginning.
ToM has long been studied within cognitive science and psychology, which is defined as an important social cognitive skill highly developed in humans and a small number of animals that involves the ability to tack both oneself and counterparts’ unobservable mental states, including but not limited to beliefs, desires, intentions, emotions, and knowledges <cite class="ltx_cite ltx_citemacro_citep">(Premack &amp; Woodruff, <a href="#bib.bib28" title="" class="ltx_ref">1978</a>; Cuzzolin et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>.
Humans naturally build rich internal ToM models of others by observing others’ behaviors, conditioning their own behaviors, and predicting the behaviors of others to forecast social interactions <cite class="ltx_cite ltx_citemacro_citep">(Oguntola et al., <a href="#bib.bib24" title="" class="ltx_ref">2023</a>)</cite>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">It has also long been argued that computational ToM, or machine ToM, is significant for AI systems and could be critical to realizing <span class="ltx_text ltx_font_bold">artificial general intelligence (AGI)</span> <cite class="ltx_cite ltx_citemacro_citep">(Bubeck et al., <a href="#bib.bib5" title="" class="ltx_ref">2023</a>; Mao et al., <a href="#bib.bib21" title="" class="ltx_ref">2023</a>; Rabinowitz et al., <a href="#bib.bib30" title="" class="ltx_ref">2018</a>)</cite>.
As an important well-established mental state model for the ToM, the <span class="ltx_text ltx_font_bold">Belief-Desire-Intention (BDI)</span> model consists of three critical components: <span class="ltx_text ltx_font_italic">beliefs</span> that represent a virtual agent’s knowledge and understanding of the current state of the world or relationships between objects and events, <span class="ltx_text ltx_font_italic">desires</span> represent the agent’s goals and preferences, and <span class="ltx_text ltx_font_italic">intentions</span> represent the actions the agent plans to take to achieve its goals <cite class="ltx_cite ltx_citemacro_citep">(Georgeff et al., <a href="#bib.bib11" title="" class="ltx_ref">1999</a>)</cite>.
Many previous studies were also conducted to model the computational ToM <cite class="ltx_cite ltx_citemacro_citep">(Nguyen &amp; González, <a href="#bib.bib23" title="" class="ltx_ref">2020</a>; Liu et al., <a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite>, and most existing works about ToM interpret belief as a binary condition as true belief and false belief <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a href="#bib.bib51" title="" class="ltx_ref">2024</a>; Gandhi et al., <a href="#bib.bib10" title="" class="ltx_ref">2023</a>; Kim et al., <a href="#bib.bib17" title="" class="ltx_ref">2023</a>)</cite>.
While many classic psychological tests of ToM heavily rely on tasks such as false-belief tasks to assess an individual’s belief about the world that contrasts with reality, some psychological studies suggest that <span class="ltx_text ltx_font_bold">belief</span> and <span class="ltx_text ltx_font_bold">confidence</span> are distinct yet equally fundamental types of mental states.
For example, as a human, <span class="ltx_text ltx_font_italic">Alice may believe Bob is on the way to school</span> or <span class="ltx_text ltx_font_italic">Alice may not believe so due to Bob’s poor credit</span>.
However, our confidence level, or credence, can be modeled as either a discrete or continuous variable, representing degrees such as fully confident, highly confident, somewhat confident, or not confident at all <cite class="ltx_cite ltx_citemacro_citep">(Bricker, <a href="#bib.bib3" title="" class="ltx_ref">2022</a>)</cite>.
Further, there is also the argument that confidence in oneself and others is equally critical <cite class="ltx_cite ltx_citemacro_citep">(Bang et al., <a href="#bib.bib1" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">Additionally, regarding ToM, the term ”orders” pertains to the number of mental state attributions needed to address a specific inquiry or contemplate a particular situation <cite class="ltx_cite ltx_citemacro_citep">(Harré, <a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>.
For instance, first-order reasoning about an individual’s representation of the world is like <span class="ltx_text ltx_font_italic">”Alice thinks that Bob likes football”</span>, while, second-order reasoning is like <span class="ltx_text ltx_font_italic">”Bob thinks that Alice believes that Bob likes football”</span>.
There could also be higher-order ToM such as third-order ToM, fourth-order ToM, etc. <cite class="ltx_cite ltx_citemacro_citep">(Wu et al., <a href="#bib.bib49" title="" class="ltx_ref">2023</a>)</cite>.
In this study, only first-order ToM and second-order ToM are considered.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Problem Statement</h3>

<div id="S2.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To streamline our analysis, we shift our attention away from expansive agent simulations, honing in on the dynamics between two generative agents grounded in the LLMs framework instead of multiple generative agents: agents <math id="S2.SS2.p1.m1" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math id="S2.SS2.p1.m2" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
These two agents participate in a dialogic exchange.
Agent <math id="S2.SS2.p1.m3" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s utterance, denoted as <math id="S2.SS2.p1.m4" class="ltx_Math" alttext="{U}_{a}" display="inline"><semantics><msub><mi>U</mi><mi>a</mi></msub><annotation encoding="application/x-tex">{U}_{a}</annotation></semantics></math>, stems from its underlying beliefs, desires, and intentions.
This triad can be captured by <math id="S2.SS2.p1.m5" class="ltx_Math" alttext="R=(B_{r},D_{r},I_{r})" display="inline"><semantics><mrow><mi>R</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>r</mi></msub><mo>,</mo><msub><mi>D</mi><mi>r</mi></msub><mo>,</mo><msub><mi>I</mi><mi>r</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">R=(B_{r},D_{r},I_{r})</annotation></semantics></math>, where <math id="S2.SS2.p1.m6" class="ltx_Math" alttext="B_{r}" display="inline"><semantics><msub><mi>B</mi><mi>r</mi></msub><annotation encoding="application/x-tex">B_{r}</annotation></semantics></math>, <math id="S2.SS2.p1.m7" class="ltx_Math" alttext="D_{r}" display="inline"><semantics><msub><mi>D</mi><mi>r</mi></msub><annotation encoding="application/x-tex">D_{r}</annotation></semantics></math> and <math id="S2.SS2.p1.m8" class="ltx_Math" alttext="I_{r}" display="inline"><semantics><msub><mi>I</mi><mi>r</mi></msub><annotation encoding="application/x-tex">I_{r}</annotation></semantics></math> represent agent <math id="S2.SS2.p1.m9" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s authentic beliefs, desires, and intentions, respectively.
In contrast, agent <math id="S2.SS2.p1.m10" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>’s utterance is represented as <math id="S2.SS2.p1.m11" class="ltx_Math" alttext="{U}_{b}" display="inline"><semantics><msub><mi>U</mi><mi>b</mi></msub><annotation encoding="application/x-tex">{U}_{b}</annotation></semantics></math>.
The beliefs, desires, and intentions of agent <math id="S2.SS2.p1.m12" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> as inferred by agent <math id="S2.SS2.p1.m13" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> are encapsulated by <math id="S2.SS2.p1.m14" class="ltx_Math" alttext="I=(B_{i},D_{i},I_{i})" display="inline"><semantics><mrow><mi>I</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><mi>i</mi></msub><mo>,</mo><msub><mi>D</mi><mi>i</mi></msub><mo>,</mo><msub><mi>I</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">I=(B_{i},D_{i},I_{i})</annotation></semantics></math>.
Agent <math id="S2.SS2.p1.m15" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> speculates on what is the real beliefs, desires, or intentions of agent <math id="S2.SS2.p1.m16" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> based on its responses and updates the perceptions and confidence accordingly.
Given that BDIs are unobservable inherently latent, and discernable only from the ongoing dialogue, our core challenge is to simulate how agents might progressively recognize each other’s genuine BDIs throughout their conversation and eventually benefit conversational communication.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>

<figure id="S3.F2" class="ltx_figure"><img src="/html/2501.15355/assets/x2.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="155" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Illustrations of module components of <span class="ltx_text ltx_font_bold">ToM aware generative agents</span> that could generate utterance according to self-BDI and tracking counterpart’s BDI.
<span class="ltx_text ltx_font_bold">Left Figure.</span> Vanilla Counterpart’s BDI Tracking Module.
<span class="ltx_text ltx_font_bold">Middle Figure.</span> Self-BDI-aware modules for generative agents.
<span class="ltx_text ltx_font_bold">Right Figure.</span> Counterpart’s BDI tracking modules with counterfactual reflection.</figcaption>
</figure>
<div id="S3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">As illustrated in <span class="ltx_text ltx_font_bold">Figure <a href="#S3.F2" title="Figure 2 ‣ 3 Methods ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>, we propose a ToM aware generative agent consisting of three main modules: <span class="ltx_text ltx_font_bold">Self-BDI aware Module</span>, <span class="ltx_text ltx_font_bold">Vanilla BDI Tracking Module</span>, and <span class="ltx_text ltx_font_bold">Counterfactual Reflection-based (CR-based) BDI Tracking Module</span>.
The self-BDI aware Module is used to generate utterances based on the agent’s own beliefs, desires, and intentions.
The Vanilla BDI Tracking Module and the CR-based Module are designed to track the counterpart’s possible BDIs and update the perceptions and confidence levels regarding these BDIs.
The former serves as a baseline for benchmarking, while the latter is a technique aimed at performance improvement.
Ideally, an agent should be capable of both generating conversations based on its own BDI and inferring and updating others’ BIDs.
However, for this study, we simplified the setup to include one agent equipped with a self-BDI-aware module and the other agent equipped with either a vanilla BDI tracking Module or a CR-based BDI tracking Module.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Self-BDI Aware Module</h3>

<div id="S3.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The episode of the conversation history is represented as <math id="S3.SS1.p1.m1" class="ltx_Math" alttext="H=({U}_{a_{1}},{U}_{b_{1}},...,{U}_{a_{i}},{U}_{b_{i}})" display="inline"><semantics><mrow><mi>H</mi><mo>=</mo><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><msub><mi>a</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>U</mi><msub><mi>b</mi><mn>1</mn></msub></msub><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><msub><mi>U</mi><msub><mi>a</mi><mi>i</mi></msub></msub><mo>,</mo><msub><mi>U</mi><msub><mi>b</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">H=({U}_{a_{1}},{U}_{b_{1}},...,{U}_{a_{i}},{U}_{b_{i}})</annotation></semantics></math>, where <math id="S3.SS1.p1.m2" class="ltx_Math" alttext="({U}_{a_{i}},{U}_{b_{i}})" display="inline"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>U</mi><msub><mi>a</mi><mi>i</mi></msub></msub><mo>,</mo><msub><mi>U</mi><msub><mi>b</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">({U}_{a_{i}},{U}_{b_{i}})</annotation></semantics></math> denotes the dialogue pair of <math id="S3.SS1.p1.m3" class="ltx_Math" alttext="i_{th}" display="inline"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">i_{th}</annotation></semantics></math> turn between agent <math id="S3.SS1.p1.m4" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and <math id="S3.SS1.p1.m5" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
During the conversation, agent <math id="S3.SS1.p1.m6" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> equipped with a self-BDI aware module is supposed to generate its utterance based on its actual BDIs <math id="S3.SS1.p1.m7" class="ltx_Math" alttext="T_{R}" display="inline"><semantics><msub><mi>T</mi><mi>R</mi></msub><annotation encoding="application/x-tex">T_{R}</annotation></semantics></math>, and the conversation history <math id="S3.SS1.p1.m8" class="ltx_Math" alttext="H_{i}" display="inline"><semantics><msub><mi>H</mi><mi>i</mi></msub><annotation encoding="application/x-tex">H_{i}</annotation></semantics></math> by prompting LLMs using the corresponding prompt <math id="S3.SS1.p1.m9" class="ltx_Math" alttext="P_{a}" display="inline"><semantics><msub><mi>P</mi><mi>a</mi></msub><annotation encoding="application/x-tex">P_{a}</annotation></semantics></math>, as is described in the following <span class="ltx_text ltx_font_bold">Equation (1)</span>.</p>
</div>
<div id="S3.SS1.p2" class="ltx_para ltx_noindent">
<table id="S3.E1" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E1X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E1X.m2" class="ltx_Math" alttext="\displaystyle{U_{a_{i+1}}}=LLM(P_{a};T_{R};H_{i})" display="inline"><semantics><mrow><msub><mi>U</mi><msub><mi>a</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><mo>=</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mi>a</mi></msub><mo>;</mo><msub><mi>T</mi><mi>R</mi></msub><mo>;</mo><msub><mi>H</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{U_{a_{i+1}}}=LLM(P_{a};T_{R};H_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
</tbody>
</table>
</div>
<section id="S3.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Zero-shot BDIs Initialization.</h4>

<div id="S3.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The initial BDIs of agent <math id="S3.SS1.SSS0.Px1.p1.m1" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> are learned from a randomly selected single episode of dialogue corpus, utilizing zero-shot prompting, without needing annotation or additional learning.
To increase randomness, we adopt an approach where the LLM generates the <math id="S3.SS1.SSS0.Px1.p1.m2" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S3.SS1.SSS0.Px1.p1.m3" class="ltx_Math" alttext="k" display="inline"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> combinations of beliefs, desires, and intentions based on the conversation episode’s history.
From these combinations, one of the combinations <math id="S3.SS1.SSS0.Px1.p1.m4" class="ltx_Math" alttext="R" display="inline"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math> is randomly selected as the initial <math id="S3.SS1.SSS0.Px1.p1.m5" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S3.SS1.SSS0.Px1.p1.m6" class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> value of BDI for the agent.
In the prompt, we have also included hints about the concepts of beliefs, desires, and intentions, emphasizing their relevance to ensure that the resulting BDI combination is closer to reality.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reverse BDIs Argumentation.</h4>

<div id="S3.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Belief often reflects personal subjectivity and may not always be correct or even morally wrong, whereas common sense tends to align with the public’s general perception.
In our experiments, we also discovered that expressing certain personalized beliefs is challenging because the conversation data tends to align more closely with common sense.
Ultimately, the features learned by the LLMs are constrained by the available data, which often results in a bias towards commonsense concepts that appear more frequently in the datasets.
To address this issue, we iteratively refine the resulting BDIs by inputting them back into the prompt and instructing the LLM to generate BDIs with the opposite meaning or a counterfactual nature.</p>
</div>
</section>
<section id="S3.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Second Order ToM Judgement.</h4>

<div id="S3.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In each round of conversation, after both parties have finished expressing themselves, the agent <math id="S3.SS1.SSS0.Px3.p1.m1" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> ponders whether the counterpart agent <math id="S3.SS1.SSS0.Px3.p1.m2" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> has understood agent <math id="S3.SS1.SSS0.Px3.p1.m3" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s BDI.
This is also determined by adding the conversation history and its own real BDI to the prompt, which in turn is generated by LLM.</p>
</div>
</section>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Vanilla BDI Tracking Module</h3>

<section id="S3.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Top-1 BDI Prompting.</h4>

<div id="S3.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The agent <math id="S3.SS2.SSS0.Px1.p1.m1" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> equipped with vanilla BDI tracking module prompts LLMs to obtain the most probable BDI combination <math id="S3.SS2.SSS0.Px1.p1.m2" class="ltx_Math" alttext="T_{I_{i}}" display="inline"><semantics><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><annotation encoding="application/x-tex">T_{I_{i}}</annotation></semantics></math> after <math id="S3.SS2.SSS0.Px1.p1.m3" class="ltx_Math" alttext="i_{th}" display="inline"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">i_{th}</annotation></semantics></math> round of interaction concludes.
Further, we select only the <math id="S3.SS2.SSS0.Px1.p1.m4" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S3.SS2.SSS0.Px1.p1.m5" class="ltx_Math" alttext="1" display="inline"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math> BDI.
Ultimately, agent <math id="S3.SS2.SSS0.Px1.p1.m6" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> generates its utterance <math id="S3.SS2.SSS0.Px1.p1.m7" class="ltx_Math" alttext="U_{b_{i+1}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><annotation encoding="application/x-tex">U_{b_{i+1}}</annotation></semantics></math> by prompting LLMs using the BDI combination with the highest confidence <math id="S3.SS2.SSS0.Px1.p1.m8" class="ltx_Math" alttext="T_{I_{i}}" display="inline"><semantics><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><annotation encoding="application/x-tex">T_{I_{i}}</annotation></semantics></math> and the dialogue history <math id="S3.SS2.SSS0.Px1.p1.m9" class="ltx_Math" alttext="H_{i}" display="inline"><semantics><msub><mi>H</mi><mi>i</mi></msub><annotation encoding="application/x-tex">H_{i}</annotation></semantics></math>, along with corresponding prompt <math id="S3.SS2.SSS0.Px1.p1.m10" class="ltx_Math" alttext="P_{b}" display="inline"><semantics><msub><mi>P</mi><mi>b</mi></msub><annotation encoding="application/x-tex">P_{b}</annotation></semantics></math>, as is described in the following <span class="ltx_text ltx_font_bold">Equation (2)</span>.</p>
</div>
<div id="S3.SS2.SSS0.Px1.p2" class="ltx_para ltx_noindent">
<table id="S3.E2" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E2X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E2X.m2" class="ltx_Math" alttext="\displaystyle{U_{b_{i+1}}}=LLM(P_{b};T_{I_{i}};H_{i})" display="inline"><semantics><mrow><msub><mi>U</mi><msub><mi>b</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><mo>=</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mi>b</mi></msub><mo>;</mo><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><mo>;</mo><msub><mi>H</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{U_{b_{i+1}}}=LLM(P_{b};T_{I_{i}};H_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>CR-based BDI Tracking Module</h3>

<div id="S3.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In this study, rather than limiting belief to specific scenarios or tasks, we disentangle belief and confidence to enable the ToM modeling in open-domain conversational interactions.
This approach allows agents to focus on personalized beliefs compared to mere common sense.
In the initial phase, <math id="S3.SS3.p1.m1" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S3.SS3.p1.m2" class="ltx_Math" alttext="k" display="inline"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> BDIs along with confidences are inferred by agent <math id="S3.SS3.p1.m3" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> based on the first utterance of agent <math id="S3.SS3.p1.m4" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
These BDIs and their associated confidence levels are then updated for <math id="S3.SS3.p1.m5" class="ltx_Math" alttext="i_{th}" display="inline"><semantics><msub><mi>i</mi><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>h</mi></mrow></msub><annotation encoding="application/x-tex">i_{th}</annotation></semantics></math> turn of communicational interaction, which is represented as <math id="S3.SS3.p1.m6" class="ltx_Math" alttext="T_{I_{i}}=((B_{i_{1}},D_{i_{1}},I_{i_{1}};C_{1}),...,(B_{i_{k}},D_{i_{k}},I_{i_{k}};C_{k}))" display="inline"><semantics><mrow><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><mo>=</mo><mrow><mo stretchy="false">(</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>D</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo>,</mo><msub><mi>I</mi><msub><mi>i</mi><mn>1</mn></msub></msub><mo>;</mo><msub><mi>C</mi><mn>1</mn></msub><mo stretchy="false">)</mo></mrow><mo>,</mo><mi mathvariant="normal">…</mi><mo>,</mo><mrow><mo stretchy="false">(</mo><msub><mi>B</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>,</mo><msub><mi>D</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>,</mo><msub><mi>I</mi><msub><mi>i</mi><mi>k</mi></msub></msub><mo>;</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">T_{I_{i}}=((B_{i_{1}},D_{i_{1}},I_{i_{1}};C_{1}),...,(B_{i_{k}},D_{i_{k}},I_{i_{k}};C_{k}))</annotation></semantics></math>, where <math id="S3.SS3.p1.m7" class="ltx_Math" alttext="C" display="inline"><semantics><mi>C</mi><annotation encoding="application/x-tex">C</annotation></semantics></math> stands for the confidence for each set of BDIs.
The agent can generate an update plan of the inferred BIDs and its corresponding confidence by prompting</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">During each round of interactions, agent <math id="S3.SS3.p2.m1" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> generates an updating plan <math id="S3.SS3.p2.m2" class="ltx_Math" alttext="L_{i+1}" display="inline"><semantics><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">L_{i+1}</annotation></semantics></math> for the inferred BDIs and confidence of agent <math id="S3.SS3.p2.m3" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> using the reflection mechanism, by prompting the LLMs with the corresponding reflection prompt <math id="S3.SS3.p2.m4" class="ltx_Math" alttext="P_{r}" display="inline"><semantics><msub><mi>P</mi><mi>r</mi></msub><annotation encoding="application/x-tex">P_{r}</annotation></semantics></math> based on the dialogue history <math id="S3.SS3.p2.m5" class="ltx_Math" alttext="H_{i}" display="inline"><semantics><msub><mi>H</mi><mi>i</mi></msub><annotation encoding="application/x-tex">H_{i}</annotation></semantics></math>.
Then, the updated BDIs and confidence levels combination <math id="S3.SS3.p2.m6" class="ltx_Math" alttext="T_{I_{i+1}}" display="inline"><semantics><msub><mi>T</mi><msub><mi>I</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><annotation encoding="application/x-tex">T_{I_{i+1}}</annotation></semantics></math> are obtained by prompting the LLMs using the corresponding prompt <math id="S3.SS3.p2.m7" class="ltx_Math" alttext="P_{r}" display="inline"><semantics><msub><mi>P</mi><mi>r</mi></msub><annotation encoding="application/x-tex">P_{r}</annotation></semantics></math>, the previous BIDs combination <math id="S3.SS3.p2.m8" class="ltx_Math" alttext="T_{I_{i}}" display="inline"><semantics><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><annotation encoding="application/x-tex">T_{I_{i}}</annotation></semantics></math>, and the updating plan <math id="S3.SS3.p2.m9" class="ltx_Math" alttext="L_{i+1}" display="inline"><semantics><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">L_{i+1}</annotation></semantics></math>, as is described in the <span class="ltx_text ltx_font_bold">Equation (3).</span></p>
</div>
<div id="S3.SS3.p3" class="ltx_para ltx_noindent">
<table id="S3.E3" class="ltx_equationgroup ltx_eqn_table">
<tbody>
<tr id="S3.E3X" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3X.m2" class="ltx_Math" alttext="\displaystyle L_{i+1}=LLM(P_{r};T_{I_{i}};H_{i})" display="inline"><semantics><mrow><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mi>r</mi></msub><mo>;</mo><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><mo>;</mo><msub><mi>H</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle L_{i+1}=LLM(P_{r};T_{I_{i}};H_{i})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="2" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr id="S3.E3Xa" class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math id="S3.E3Xa.m2" class="ltx_Math" alttext="\displaystyle{T_{I_{i+1}}}=LLM(P_{u};L_{i+1};T_{I_{i}})" display="inline"><semantics><mrow><msub><mi>T</mi><msub><mi>I</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></msub><mo>=</mo><mrow><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>L</mi><mo lspace="0em" rspace="0em">​</mo><mi>M</mi><mo lspace="0em" rspace="0em">​</mo><mrow><mo stretchy="false">(</mo><msub><mi>P</mi><mi>u</mi></msub><mo>;</mo><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>;</mo><msub><mi>T</mi><msub><mi>I</mi><mi>i</mi></msub></msub><mo stretchy="false">)</mo></mrow></mrow></mrow><annotation encoding="application/x-tex">\displaystyle{T_{I_{i+1}}}=LLM(P_{u};L_{i+1};T_{I_{i}})</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">After reflection and updating, the agent <math id="S3.SS3.p4.m1" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> generates its utterance by prompting LLMs using the BDI set with the highest confidence along with the corresponding prompt, which is similar to <span class="ltx_text ltx_font_bold">Equation (2).</span></p>
</div>
<section id="S3.SS3.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Reflection.</h4>

<div id="S3.SS3.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection is an effective reinforcement technique for LLMs-based agents, which can be a reinforcement learning way via verbal feedback without tuning the parameters of LLMs or devising a reword function <cite class="ltx_cite ltx_citemacro_citep">(Shinn et al., <a href="#bib.bib40" title="" class="ltx_ref">2023</a>)</cite>.
It employs a persisting memory of self-refective experiences, allowing an agent to revisit its errors and make improved decisions in subsequent iterations.
It consists of three distinct models: an actor model, an evaluator model, and a self-reflection model, in which the evaluator model plays a crucial role in assessing the quality.
We aim to update the <math id="S3.SS3.SSS0.Px1.p1.m1" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S3.SS3.SSS0.Px1.p1.m2" class="ltx_Math" alttext="k" display="inline"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> BDIs and the related confidence level using the reflection of LLMs.
However, since the BDI is unobservable, it results in the established reflection cannot directly evaluate the similarity between the inferred BDI and the actual BDI.
To solve this problem, we propose a counterfactual reflection based on foresight and counterfactual thinking.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Foresight.</h4>

<div id="S3.SS3.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">It is argued in previous studies that <span class="ltx_text ltx_font_italic">foresight</span> and <span class="ltx_text ltx_font_italic">reflection</span> are equally critical for machine ToM <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a href="#bib.bib54" title="" class="ltx_ref">2023</a>)</cite>.
To solve the problem that BDI is unobservable, we compared the observable utterances with predicted utterances instead:
in each interaction round, we let the agent <math id="S3.SS3.SSS0.Px2.p1.m1" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> predict the agent <math id="S3.SS3.SSS0.Px2.p1.m2" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s utterance <math id="S3.SS3.SSS0.Px2.p1.m3" class="ltx_Math" alttext="U_{a_{p}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>p</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{p}}</annotation></semantics></math> by prompting the LLM, utilizing inferred BDIs combination with the highest confidence <math id="S3.SS3.SSS0.Px2.p1.m4" class="ltx_Math" alttext="T_{I}" display="inline"><semantics><msub><mi>T</mi><mi>I</mi></msub><annotation encoding="application/x-tex">T_{I}</annotation></semantics></math> and conversation history <math id="S3.SS3.SSS0.Px2.p1.m5" class="ltx_Math" alttext="H" display="inline"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>.
After agent <math id="S3.SS3.SSS0.Px2.p1.m6" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s real utterance <math id="S3.SS3.SSS0.Px2.p1.m7" class="ltx_Math" alttext="U_{a_{r}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>r</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{r}}</annotation></semantics></math> is observed, agent <math id="S3.SS3.SSS0.Px2.p1.m8" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> compares the <math id="S3.SS3.SSS0.Px2.p1.m9" class="ltx_Math" alttext="U_{a_{p}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>p</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{p}}</annotation></semantics></math> with <math id="S3.SS3.SSS0.Px2.p1.m10" class="ltx_Math" alttext="U_{a_{r}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>r</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{r}}</annotation></semantics></math> by scoring the two sentences with a decimal value <math id="S3.SS3.SSS0.Px2.p1.m11" class="ltx_Math" alttext="S" display="inline"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> between <math id="S3.SS3.SSS0.Px2.p1.m12" class="ltx_Math" alttext="{[0,1]}" display="inline"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">{[0,1]}</annotation></semantics></math> to evaluate their similarity.</p>
</div>
</section>
<section id="S3.SS3.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Counterfactual Reflection.</h4>

<div id="S3.SS3.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Inspired by the argument in the previous study that <span class="ltx_text ltx_font_italic">counterfactual thinking</span> may be critical for an individual to understand others by predicting what action they will take in a similar situation <cite class="ltx_cite ltx_citemacro_citep">(Cuzzolin et al., <a href="#bib.bib7" title="" class="ltx_ref">2020</a>)</cite>, we propose a counterfactual reflection.
The proposed counterfactual reflection is conducted in the following steps:
If the <math id="S3.SS3.SSS0.Px3.p1.m1" class="ltx_Math" alttext="S_{i+1}" display="inline"><semantics><msub><mi>S</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">S_{i+1}</annotation></semantics></math> increases compared with <math id="S3.SS3.SSS0.Px3.p1.m2" class="ltx_Math" alttext="S_{i}" display="inline"><semantics><msub><mi>S</mi><mi>i</mi></msub><annotation encoding="application/x-tex">S_{i}</annotation></semantics></math>, the agent <math id="S3.SS3.SSS0.Px3.p1.m3" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> reflects on the previously inferred BDIs of agent <math id="S3.SS3.SSS0.Px3.p1.m4" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> based on the evaluation value <math id="S3.SS3.SSS0.Px3.p1.m5" class="ltx_Math" alttext="S" display="inline"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math> on the <math id="S3.SS3.SSS0.Px3.p1.m6" class="ltx_Math" alttext="U_{a_{p}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>p</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{p}}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px3.p1.m7" class="ltx_Math" alttext="U_{a_{r}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>r</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{r}}</annotation></semantics></math> by prompting the LLMs.
Further, agent <math id="S3.SS3.SSS0.Px3.p1.m8" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> reflects that <span class="ltx_text ltx_font_italic">”what if my previously inferred BDI of agent <math id="S3.SS3.SSS0.Px3.p1.m9" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> is not correct?”</span>.
Then agent <math id="S3.SS3.SSS0.Px3.p1.m10" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> carries on the conversation with itself and generates a virtual response <math id="S3.SS3.SSS0.Px3.p1.m11" class="ltx_Math" alttext="U_{a_{v}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>v</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{v}}</annotation></semantics></math> instead of agent <math id="S3.SS3.SSS0.Px3.p1.m12" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> and compares the <math id="S3.SS3.SSS0.Px3.p1.m13" class="ltx_Math" alttext="U_{a_{v}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>v</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{v}}</annotation></semantics></math> with the real one <math id="S3.SS3.SSS0.Px3.p1.m14" class="ltx_Math" alttext="U_{a_{r}}" display="inline"><semantics><msub><mi>U</mi><msub><mi>a</mi><mi>r</mi></msub></msub><annotation encoding="application/x-tex">U_{a_{r}}</annotation></semantics></math> to obtain a virtual score <math id="S3.SS3.SSS0.Px3.p1.m15" class="ltx_Math" alttext="S_{v}" display="inline"><semantics><msub><mi>S</mi><mi>v</mi></msub><annotation encoding="application/x-tex">S_{v}</annotation></semantics></math>.
If the <math id="S3.SS3.SSS0.Px3.p1.m16" class="ltx_Math" alttext="S_{v}&gt;S" display="inline"><semantics><mrow><msub><mi>S</mi><mi>v</mi></msub><mo>&gt;</mo><mi>S</mi></mrow><annotation encoding="application/x-tex">S_{v}&gt;S</annotation></semantics></math>, then update the BDI and generate the <math id="S3.SS3.SSS0.Px3.p1.m17" class="ltx_Math" alttext="U_{b}" display="inline"><semantics><msub><mi>U</mi><mi>b</mi></msub><annotation encoding="application/x-tex">U_{b}</annotation></semantics></math> for the next round, otherwise, update the BIDs and confidence level using the <math id="S3.SS3.SSS0.Px3.p1.m18" class="ltx_Math" alttext="T_{I}" display="inline"><semantics><msub><mi>T</mi><mi>I</mi></msub><annotation encoding="application/x-tex">T_{I}</annotation></semantics></math> and <math id="S3.SS3.SSS0.Px3.p1.m19" class="ltx_Math" alttext="H" display="inline"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>.</p>
</div>
</section>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experiment Setup</h3>

<div id="S4.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">By conducting experiments, we try to answer the following research questions:</p>
</div>
<div id="S4.SS1.p2" class="ltx_para ltx_noindent">
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RQ1</span>: To what extent could the ToM-aware generative agent infer about other generative agents’ unobservable beliefs, desires, and intentions during open-domain conversational interactions? (First-order ToM)</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RQ2</span>: To what extent could the ToM-aware generative agent infer about counterpart agents’ understanding of its own beliefs, desires, and intentions during open-domain conversational interactions? (Second-order ToM)</p>
</div>
</li>
<li id="S4.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S4.I1.i3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">RQ3</span>: To what extent could the ToM-aware generative agent benefit downstream tasks of open-domain conversational interactions in short-term interactions?</p>
</div>
</li>
</ul>
</div>
<div id="S4.SS1.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">We use these two downstream tasks to evaluate our method: empathetic dialogue and persuasive dialogue, which are highly related to ToM mental status modeling during human beings’ interactions.
We can abstract downstream tasks in the dialog domain into specific goals.
For instance, in empathetic dialogue, one agent’s goal is to satisfy the other emotionally.
While, in persuasive dialogue, one agent aims to persuade the other to make a decision, such as purchasing a certain good or service, or making a donation.
<span class="ltx_text ltx_font_bold">Empathetic Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Rashkin et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib32" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span> and <span class="ltx_text ltx_font_bold">Persuasion Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Wang et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib48" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span> are two dialogue datasets publicly available and the details about these datasets can be found in the Appendix.</p>
</div>
<div id="S4.SS1.p4" class="ltx_para ltx_noindent">
<p class="ltx_p">In the simulation, one agent plays the role of either an Empathetic-needing NPC or a Persuadee NPC, taking actions as the previously introduced agent <math id="S4.SS1.p4.m1" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>.
Meanwhile, another agent plays the role of either an Empathetic NPC or a Persuador NPC, taking actions as the agent <math id="S4.SS1.p4.m2" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math>.
The initialization of BDIs is conducted on 100 dialogue episodes randomly sampled from the above two datasets.
Similarly, for each experiment, the two agents interact with each other until agent A believes that agent B understands its BDI, at which point the dialog is considered successfully concluded.
However, if the conditions for ending the dialog are not met within <math id="S4.SS1.p4.m3" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> rounds, the dialog is considered unsuccessful.
We mainly evaluated two versions of LLMs as generative agents: GPT-4 (gpt-4-0125-preview) and GPT-3.5 (gpt-3.5-turbo-0125).
OpenAI davinci model (text-similarity-davinci-001) is
used for scoring the similarity of the predicted utterance and the real utterance.
In this study, the BDIs set number <math id="S4.SS1.p4.m4" class="ltx_Math" alttext="top" display="inline"><semantics><mrow><mi>t</mi><mo lspace="0em" rspace="0em">​</mo><mi>o</mi><mo lspace="0em" rspace="0em">​</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">top</annotation></semantics></math>-<math id="S4.SS1.p4.m5" class="ltx_Math" alttext="k" display="inline"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math> is set to 3 and the maximum number of turns in each dialogue episode <math id="S4.SS1.p4.m6" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> is set to 10.</p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>BDI Infering Evaluation (First-order ToM)</h3>

<div id="S4.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To answer <span class="ltx_text ltx_font_bold">RQ1</span>, we conducted an experiment in which two agents engaged in 100 conversation rounds.
One agent implemented only the self-BDI-aware module and generated conversations based on its initial BDIs.
The other agent used either the vanilla BDI tracing module or the CR-based BDI tracing module, referred to as Vanilla and ToM + CR, respectively.
At the end of the dialog, we recorded the set pairs of the inferred BDI and the true BDI.
Afterward, we asked three annotators to evaluate each pair to assess the similarity between the inferred and actual BDI and score within the range of [0, 5].
The average score is calculated based on the scores given by three annotators to decide whether the inferred BDI and the real BDI are similar (<math id="S4.SS2.p1.m1" class="ltx_Math" alttext="&gt;0.25" display="inline"><semantics><mrow><mi></mi><mo>&gt;</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">&gt;0.25</annotation></semantics></math>) or not similar (<math id="S4.SS2.p1.m2" class="ltx_Math" alttext="&lt;0.25" display="inline"><semantics><mrow><mi></mi><mo>&lt;</mo><mn>0.25</mn></mrow><annotation encoding="application/x-tex">&lt;0.25</annotation></semantics></math>).
Then we calculate the precision, F1 score, and recall score from the aspects of belief, desire, and intention, respectively.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The performance evaluation results of our proposal for BDI tracking paradigm as <span class="ltx_text ltx_font_bold">the first-order ToM</span> on the <span class="ltx_text ltx_font_bold">Empathecit Dialogue</span> dataset and the <span class="ltx_text ltx_font_bold">Persuasion Dialogue</span> dataset. The LLMs used are GPT-3.5 or GPT-4. P, F, and R denote precision, f1-score, and recall respectively.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:377.6pt;height:139.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.7pt,11.6pt) scale(0.856255975484115,0.856255975484115) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span class="ltx_text ltx_font_bold">Empathetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span class="ltx_text ltx_font_bold">Persuasion</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-3.5-turbo</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-3.5-turbo</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_t">R</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (B)</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.42</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.36</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.21</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.51</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.40</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.36</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.28</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">ToM + CR (B)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.47</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.45</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.46</span></td>
<td class="ltx_td ltx_align_center">0.29</td>
<td class="ltx_td ltx_align_center">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_rr">0.26</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.55</span></td>
<td class="ltx_td ltx_align_center">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.47</span></td>
<td class="ltx_td ltx_align_center">0.40</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.54</span></td>
<td class="ltx_td ltx_align_center">0.46</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (D)</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.34</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.16</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.25</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">ToM + CR (D)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.38</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.39</span></td>
<td class="ltx_td ltx_align_center">0.22</td>
<td class="ltx_td ltx_align_center">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_rr">0.20</td>
<td class="ltx_td ltx_align_center">0.53</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.51</span></td>
<td class="ltx_td ltx_align_center">0.28</td>
<td class="ltx_td ltx_align_center">0.32</td>
<td class="ltx_td ltx_align_center">0.30</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (I)</th>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.29</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.18</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.21</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">ToM + CR (I)</th>
<td class="ltx_td ltx_align_center ltx_border_bb">0.33</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.31</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr">0.19</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.32</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.35</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.34</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS2.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">From <span class="ltx_text ltx_font_bold">Table <a href="#S4.T1" title="Table 1 ‣ 4.2 BDI Infering Evaluation (First-order ToM) ‣ 4 Experiments ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>, we can summarize that while individual exceptions may exist, overall, GPT-4 demonstrates superior performance in the first-order ToM for inferring BDIs compared to GPT-3.5 when provided with the same premises.
Additionally, for both GPT-4 and GPT-3.5, we observe that the overall ToM + CR approach has a better performance compared with Vanilla’s approach.
The specific results of the experiments also demonstrate the effectiveness of the proposed method in first-order ToM detection across two downstream tasks.</p>
</div>
</section>
<section id="S4.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>BDI Infering Evaluation (Second-order ToM)</h3>

<div id="S4.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To address <span class="ltx_text ltx_font_bold">RQ2</span>, we assess the second-order ToM on the two conversation datasets separately, as whether a conversation meets the end criterion is determined by agent <math id="S4.SS3.p1.m1" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>’s second-order ToM.
Likewise, from the 100 rounds of conversation conducted by the two agents, we ask three annotators to judge whether agent <math id="S4.SS3.p1.m2" class="ltx_Math" alttext="A" display="inline"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math> believes that agent <math id="S4.SS3.p1.m3" class="ltx_Math" alttext="B" display="inline"><semantics><mi>B</mi><annotation encoding="application/x-tex">B</annotation></semantics></math> understands its BDI.
Similar to the first-order ToM, the evaluation of the second-order ToM can also be treated as a binary classification problem, thus the precision, F1 score, and recall score from the aspects of belief, desire, and intention are evaluated, respectively.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>The performance evaluation results of our proposal for BDI tracking paradigm as <span class="ltx_text ltx_font_bold">the second-order ToM</span> on the <span class="ltx_text ltx_font_bold">Empathecit Dialogue</span> dataset and the <span class="ltx_text ltx_font_bold">Persuasion Dialogue</span> dataset. The LLMs used are GPT-3.5 or GPT-4. P, F, and R denote precision, f1-score, and recall respectively.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:377.6pt;height:139.6pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-31.7pt,11.6pt) scale(0.856255975484115,0.856255975484115) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span class="ltx_text ltx_font_bold">Empathetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="6"><span class="ltx_text ltx_font_bold">Persuasion</span></td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-3.5-turbo</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="3">GPT-3.5-turbo</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_th ltx_th_row"></th>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">R</td>
<td class="ltx_td ltx_align_center ltx_border_t">P</td>
<td class="ltx_td ltx_align_center ltx_border_t">F</td>
<td class="ltx_td ltx_align_center ltx_border_t">R</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (B)</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.25</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span class="ltx_text ltx_font_bold">0.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.37</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.38</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">ToM + CR (B)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.56</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.35</span></td>
<td class="ltx_td ltx_align_center">0.26</td>
<td class="ltx_td ltx_align_center">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_rr">0.24</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.38</span></td>
<td class="ltx_td ltx_align_center">0.38</td>
<td class="ltx_td ltx_align_center ltx_border_r">0.38</td>
<td class="ltx_td ltx_align_center">0.36</td>
<td class="ltx_td ltx_align_center">0.36</td>
<td class="ltx_td ltx_align_center">0.36</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (D)</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.17</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.18</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.25</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row">ToM + CR (D)</th>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.27</span></td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.27</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.27</span></td>
<td class="ltx_td ltx_align_center">0.21</td>
<td class="ltx_td ltx_align_center">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_rr">0.23</td>
<td class="ltx_td ltx_align_center">0.29</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_r"><span class="ltx_text ltx_font_bold">0.31</span></td>
<td class="ltx_td ltx_align_center">0.24</td>
<td class="ltx_td ltx_align_center">0.29</td>
<td class="ltx_td ltx_align_center">0.26</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t">Vanilla (I)</th>
<td class="ltx_td ltx_align_center ltx_border_t">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.16</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.15</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.16</td>
<td class="ltx_td ltx_align_center ltx_border_t"><span class="ltx_text ltx_font_bold">0.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_t">0.23</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0.27</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.30</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.25</td>
</tr>
<tr class="ltx_tr">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb">ToM + CR (I)</th>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text ltx_font_bold">0.54</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.20</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.22</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr">0.21</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r"><span class="ltx_text ltx_font_bold">0.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.28</td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.26</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">From <span class="ltx_text ltx_font_bold">Table <a href="#S4.T2" title="Table 2 ‣ 4.3 BDI Infering Evaluation (Second-order ToM) ‣ 4 Experiments ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a></span>, we conclude that GPT-4 demonstrates overall superior performance in the second-order ToM perception compared to GPT-3.5 when provided with the same premises.
Additionally, for both GPT-4 and GPT-3.5, we observe that the overall ToM + CR approach has a better performance compared with Vanilla’s approach for most evaluations.
The effectiveness of the proposed method in second-order ToM detection across two downstream tasks could also be demonstrated by the specific results.</p>
</div>
</section>
<section id="S4.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Dialogue Generation Evaluation (Down-stream tasks)</h3>

<div id="S4.SS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">As is argued that previous studies of emotional support or negotiation dialogue can evaluate the turn-level performance using the fixed reference responses of a benchmark corpus, however, it is better to evaluate the dialogue level of proactive dialogue systems using automatic metrics: average turn (AT) and the success rate at turn <math id="S4.SS4.p1.m1" class="ltx_Math" alttext="t" display="inline"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math> (SR<math id="S4.SS4.p1.m2" class="ltx_Math" alttext="@t" display="inline"><semantics><mrow><mi mathvariant="normal">@</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">@t</annotation></semantics></math>) <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a href="#bib.bib8" title="" class="ltx_ref">2023</a>)</cite>.
We hypothesize that an agent who has insight into another agent’s BDI can accomplish the goal of the conversation more quickly within a pre-defined maximum turn, or efficiently when performing downstream tasks than the agent without the ToM mechanism.
The maximum turn of the conversation is set as 10 in our experiment.</p>
</div>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Experimental results for downstream tasks: <span class="ltx_text ltx_font_bold">Empathecit Dialogue</span> and <span class="ltx_text ltx_font_bold">Persuasion Dialogue</span>. Two LLMs are evaluated accordingly: GPT-3.5 and GPT-4.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:377.6pt;height:108.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-32.8pt,9.3pt) scale(0.85201653242145,0.85201653242145) ;">
<table class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Empathetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4"><span class="ltx_text ltx_font_bold">Persuasion</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2">GPT-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2">GPT-4</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2">GPT-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" colspan="2">GPT-4</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td"></td>
<td class="ltx_td ltx_align_center ltx_border_t">AT <math id="S4.T3.m1" class="ltx_Math" alttext="\downarrow" display="inline"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">SR@t <math id="S4.T3.m2" class="ltx_Math" alttext="\uparrow" display="inline"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">AT <math id="S4.T3.m3" class="ltx_Math" alttext="\downarrow" display="inline"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">SR@t <math id="S4.T3.m4" class="ltx_Math" alttext="\uparrow" display="inline"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">AT <math id="S4.T3.m5" class="ltx_Math" alttext="\downarrow" display="inline"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">SR@t <math id="S4.T3.m6" class="ltx_Math" alttext="\uparrow" display="inline"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">AT <math id="S4.T3.m7" class="ltx_Math" alttext="\downarrow" display="inline"><semantics><mo stretchy="false">↓</mo><annotation encoding="application/x-tex">\downarrow</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_t">SR@t <math id="S4.T3.m8" class="ltx_Math" alttext="\uparrow" display="inline"><semantics><mo stretchy="false">↑</mo><annotation encoding="application/x-tex">\uparrow</annotation></semantics></math>
</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_t">Without ToM (Baseline)</td>
<td class="ltx_td ltx_align_center ltx_border_t">6.72</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.41</td>
<td class="ltx_td ltx_align_center ltx_border_t">6.53</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">0.43</td>
<td class="ltx_td ltx_align_center ltx_border_t">6.32</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.38</td>
<td class="ltx_td ltx_align_center ltx_border_t">5.51</td>
<td class="ltx_td ltx_align_center ltx_border_t">0.45</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_tt">Vanilla ToM</td>
<td class="ltx_td ltx_align_center ltx_border_tt">6.92</td>
<td class="ltx_td ltx_align_center ltx_border_tt">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_tt">6.24</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt">0.45</td>
<td class="ltx_td ltx_align_center ltx_border_tt">6.05</td>
<td class="ltx_td ltx_align_center ltx_border_tt">0.39</td>
<td class="ltx_td ltx_align_center ltx_border_tt">5.92</td>
<td class="ltx_td ltx_align_center ltx_border_tt">0.49</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center">Reflection-based ToM</td>
<td class="ltx_td ltx_align_center">6.73</td>
<td class="ltx_td ltx_align_center"><span class="ltx_text ltx_font_bold">0.48</span></td>
<td class="ltx_td ltx_align_center">6.08</td>
<td class="ltx_td ltx_align_center ltx_border_rr">0.51</td>
<td class="ltx_td ltx_align_center">5.96</td>
<td class="ltx_td ltx_align_center">0.43</td>
<td class="ltx_td ltx_align_center">5.71</td>
<td class="ltx_td ltx_align_center">0.41</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb">CR-based ToM</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">6.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb">0.53</td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">5.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr"><span class="ltx_text ltx_font_bold">0.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">5.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">5.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb"><span class="ltx_text ltx_font_bold">0.55</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div id="S4.SS4.p2" class="ltx_para ltx_noindent">
<p class="ltx_p">From the results in <span class="ltx_text ltx_font_bold">Table <a href="#S4.T3" title="Table 3 ‣ 4.4 Dialogue Generation Evaluation (Down-stream tasks) ‣ 4 Experiments ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span>, the conclusion could be made that both AT and SR<math id="S4.SS4.p2.m1" class="ltx_Math" alttext="@t" display="inline"><semantics><mrow><mi mathvariant="normal">@</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">@t</annotation></semantics></math> have a better performance for agents with ToM than without ToM, either GPT-3.5 or GPT-4 on both datasets.
Reflection-based ToM significantly outperforms Vanilla ToM for both AT and SR<math id="S4.SS4.p2.m2" class="ltx_Math" alttext="@t" display="inline"><semantics><mrow><mi mathvariant="normal">@</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">@t</annotation></semantics></math>, while CR-based ToM achieves the best results on each dataset.
Even in the evaluation of SR<math id="S4.SS4.p2.m3" class="ltx_Math" alttext="@t" display="inline"><semantics><mrow><mi mathvariant="normal">@</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">@t</annotation></semantics></math> for GPT-3.5, where Reflection-based ToM slightly outperforms CR-based ToM, it’s important to note that SR<math id="S4.SS4.p2.m4" class="ltx_Math" alttext="@t" display="inline"><semantics><mrow><mi mathvariant="normal">@</mi><mo lspace="0em" rspace="0em">​</mo><mi>t</mi></mrow><annotation encoding="application/x-tex">@t</annotation></semantics></math> is influenced not only by the model itself but also by the perceptual effect of second-order ToM based on LLMs.
As Reflection-based ToM is the <span class="ltx_text ltx_font_bold">ToM-Agent</span> without CR, these evaluations can also serve as an ablation study on each part of the <span class="ltx_text ltx_font_bold">ToM-Agent</span>.</p>
</div>
<div id="S4.SS4.p3" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">More details of the prompt templates and dialogue examples can be found in the Appendix.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Observations</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Observations of Good Examples</h3>

<figure id="S5.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_block ltx_minipage ltx_align_middle" style="width:170.7pt;">
<img src="/html/2501.15355/assets/graphics/3_2.png" id="S5.F3.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="383" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>Good Dialogue Examples for BDI Inference</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_block ltx_minipage ltx_align_middle" style="width:170.7pt;">
<img src="/html/2501.15355/assets/x3.png" id="S5.F3.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="436" height="246" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>Numerical Changes in confidence for BDI</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(a) Good Examples for BDI Infere. (b)Illustrations of curves of numerical changes in confidence for belief, desire, and intention in an episodic dialogue. As the dialogue progresses, the confidence values for belief, desire, and intention all increase steadily and eventually stabilize at high levels.</figcaption>
</figure>
<div id="S5.SS1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">During the dialogue, we use the text-embedding-3-large model of GPT to compare the similarity between the inferred BDI (Belief, Desire, Intention) and the true BDI by calculating the cosine similarity between the two embeddings.
As shown in the <span class="ltx_text ltx_font_bold">Figure <a href="#S5.F3" title="Figure 3 ‣ 5.1 Observations of Good Examples ‣ 5 Observations ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a></span>, this can be seen as a good example of confidence changes during the conversation for ToM simulation.
This result aligns well with our expectations: The steady increase in confidence values for belief, desire, and intention as the dialogue progresses, along with the fluctuations in the middle, accurately reflects the dynamic and evolving nature of these mental states during an interaction.
The oscillations represent periods of uncertainty or shifts in perspective, which eventually resolve, leading to higher confidence levels by the dialogue’s conclusion.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Observations of ”Bad” Examples</h3>

<figure id="S5.F4" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf1" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_block ltx_minipage ltx_align_middle" style="width:170.7pt;">
<img src="/html/2501.15355/assets/graphics/4_1.png" id="S5.F4.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="349" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(a) </span>”Bad” Dialogue Examples 1</figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S5.F4.sf2" class="ltx_figure ltx_figure_panel ltx_align_center">
<div class="ltx_block ltx_minipage ltx_align_middle" style="width:170.7pt;">
<img src="/html/2501.15355/assets/graphics/4_2.png" id="S5.F4.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="338" alt="Refer to caption">
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">(b) </span>”Bad” Dialogue Examples 2</figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Illustrations of curves of numerical changes in confidence for belief, desire, and intention in an episodic dialogue.
<span class="ltx_text ltx_font_bold">Figure (a).</span> As the dialogue progresses, the confidence values for Belief, Desire, and Intention all initially increase but eventually settle at the lower end of the scale.
<span class="ltx_text ltx_font_bold">Figure (b).</span> As the dialogue progresses, the confidence values for desire and intention increase steadily and eventually stabilize at high levels but belief eventually settles at the lower end of the scale.</figcaption>
</figure>
<div id="S5.SS2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">As shown in the (a) and (b) of <span class="ltx_text ltx_font_bold">Figure <a href="#S5.F4" title="Figure 4 ‣ 5.2 Observations of ”Bad” Examples ‣ 5 Observations ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span>, these can be seen as examples of suboptimal confidence changes during the conversation for ToM simulation.
In Figure (a), the confidence values for Belief, Desire, and Intention initially increase but eventually settle at the lower end of the scale.
In Figure (b), the confidence values for Desire and Intention increase steadily and eventually stabilize at high levels, while Belief settles at a lower level.
Ending a dialogue when confidence levels are not at their maximum is generally considered a suboptimal outcome.
However, since the purpose of the study was to simulate people’s BDI (Belief, Desire, and Intention) confidence levels about others during dialogue—and recognizing that people often make decisions in conversations without their confidence being at its peak—these results should be viewed in a more positive light.
The observed fluctuations and eventual increase in confidence levels accurately mirror natural decision-making processes in real-life interactions, where individuals often proceed despite varying levels of certainty.
This alignment with realistic human behavior suggests that our simulation effectively captures the dynamics of BDI confidence during dialogue.
However, if the goal is to develop super-AIs with capabilities that surpass the theory of mind of human beings, these results might be seen as suboptimal.</p>
</div>
</section>
<section id="S5.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Ethical Concerns</h3>

<div id="S5.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">As the conversation examples illustrated in <span class="ltx_text ltx_font_bold">Figure <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a></span>, someone may think that the implementation of this kind of agent may raise ethical concerns.
However, we originally chose this nearly extreme example because our research focuses on the broader context of generative agents, and we wanted to demonstrate that ToM agents can take into account and imitate human beliefs in their interactions.
This distinction is crucial because human beliefs do not always follow common sense (may even be morally incorrect), and such discrepancies can significantly impact agent behavior.
Instead of labeling the act of mimicking human beliefs as inherently unethical, we believe it is essential to recognize this potential issue through our research.
Our goal is to provide a foundation for the research community to discuss and study the implications of such behavior in ToM agents further.
We hope our work stimulates constructive discourse and exploration into how generative agents can better understand and ethically interact with human beliefs.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In this study, we proposed a novel paradigm <span class="ltx_text ltx_font_bold">ToM-Agent</span> that equips LLMs-based generative agents with ToM reasoning, allowing them to emulate cognitive mental states such as beliefs, desires, and intentions (BDIs) during open-domain conversational interactions.
The counterfactual reflection method is also proposed to dynamically adjust confidence levels related to inferred BDIs of counterparts based on past conversation history, to reflect on the gap between predicted responses of counterparts and their real utterances, which enhances the confidence updating performances of inferred BDIs.
Leveraging datasets from empathetic and persuasion dialogue research, we evaluate the performance of our proposed agent architecture in downstream tasks.
Finally, the results show that equipping the generative agents with ToM is reasonable and will benefit the downstream task in the long term.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al. (2022)</span>
<span class="ltx_bibblock">
Dan Bang, Rani Moran, Nathaniel D. Daw, and Stephen M. Fleming.

</span>
<span class="ltx_bibblock">Neurocomputational mechanisms of confidence in self and others.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature communications</em>, 13(1), December 2022.

</span>
<span class="ltx_bibblock">ISSN 2041-1723.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41467-022-31674-w</span>.

</span>
<span class="ltx_bibblock">Publisher Copyright: © 2022, The Author(s).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bara et al. (2021)</span>
<span class="ltx_bibblock">
Cristian-Paul Bara, Sky CH-Wang, and Joyce Chai.

</span>
<span class="ltx_bibblock">MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pp.  1112–1125, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.85</span>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bricker (2022)</span>
<span class="ltx_bibblock">
Adam Michael Bricker.

</span>
<span class="ltx_bibblock">I Hear You Feel Confident.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">The Philosophical Quarterly</em>, 73(1):24–43, 03 2022.

</span>
<span class="ltx_bibblock">ISSN 0031-8094.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1093/pq/pqac007</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1093/pq/pqac007" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1093/pq/pqac007</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock">In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), <em class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>, volume 33, pp.  1877–1901. Curran Associates, Inc., 2020.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bubeck et al. (2023)</span>
<span class="ltx_bibblock">
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang.

</span>
<span class="ltx_bibblock">Sparks of artificial general intelligence: Early experiments with gpt-4, 2023.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chandrasekaran et al. (2017)</span>
<span class="ltx_bibblock">
Arjun Chandrasekaran, Deshraj Yadav, Prithvijit Chattopadhyay, Viraj Prabhu, and Devi Parikh.

</span>
<span class="ltx_bibblock">It takes two to tango: Towards theory of ai’s mind, 2017.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cuzzolin et al. (2020)</span>
<span class="ltx_bibblock">
F. Cuzzolin, A. Morelli, B. Cîrstea, and B. J. Sahakian.

</span>
<span class="ltx_bibblock">Knowing me, knowing you: theory of mind in ai.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Psychological Medicine</em>, 50(7):1057–1061, 2020.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1017/S0033291720000835</span>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2023)</span>
<span class="ltx_bibblock">
Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Plug-and-play policy planner for large language model powered dialogue agents, 2023.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dvash &amp; Shamay-Tsoory (2014)</span>
<span class="ltx_bibblock">
Jonathan Dvash and Simone Shamay-Tsoory.

</span>
<span class="ltx_bibblock">Theory of mind and empathy as multidimensional constructs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Topics in Language Disorders</em>, 34:282–295, 12 2014.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1097/TLD.0000000000000040</span>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gandhi et al. (2023)</span>
<span class="ltx_bibblock">
Kanishk Gandhi, Jan-Philipp Fränken, Tobias Gerstenberg, and Noah D. Goodman.

</span>
<span class="ltx_bibblock">Understanding social reasoning in language models with language models, 2023.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Georgeff et al. (1999)</span>
<span class="ltx_bibblock">
Michael Georgeff, Barney Pell, Martha Pollack, Milind Tambe, and Michael Wooldridge.

</span>
<span class="ltx_bibblock">The belief-desire-intention model of agency.

</span>
<span class="ltx_bibblock">In Jörg P. Müller, Anand S. Rao, and Munindar P. Singh (eds.), <em class="ltx_emph ltx_font_italic">Intelligent Agents V: Agents Theories, Architectures, and Languages</em>, pp.  1–10, Berlin, Heidelberg, 1999. Springer Berlin Heidelberg.

</span>
<span class="ltx_bibblock">ISBN 978-3-540-49057-9.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grazzani et al. (2018)</span>
<span class="ltx_bibblock">
Ilaria Grazzani, Veronica Ornaghi, Elisabetta Conte, Alessandro Pepe, and Claudia Caprin.

</span>
<span class="ltx_bibblock">The relation between emotion understanding and theory of mind in children aged 3 to 8: The key role of language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Frontiers in Psychology</em>, 9, 2018.

</span>
<span class="ltx_bibblock">ISSN 1664-1078.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/fpsyg.2018.00724</span>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Jiaxian Guo, Bo Yang, Paul Yoo, Yuchen Lin, Yusuke Iwasawa, and Yutaka Matsuo.

</span>
<span class="ltx_bibblock">Suspicion-agent: Playing imperfect information games with theory of mind aware gpt4, 2023.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harré (2022)</span>
<span class="ltx_bibblock">
Michael S. Harré.

</span>
<span class="ltx_bibblock">What can game theory tell us about an ai ‘theory of mind’?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Games</em>, 13(3), 2022.

</span>
<span class="ltx_bibblock">ISSN 2073-4336.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/g13030046</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://www.mdpi.com/2073-4336/13/3/46" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.mdpi.com/2073-4336/13/3/46</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Chuanbo Hu, Bin Liu, Xin Li, and Yanfang Ye.

</span>
<span class="ltx_bibblock">Unveiling the potential of knowledge-prompted chatgpt for enhancing drug trafficking detection on social media, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jamali et al. (2023)</span>
<span class="ltx_bibblock">
Mohsen Jamali, Ziv M. Williams, and Jing Cai.

</span>
<span class="ltx_bibblock">Unveiling theory of mind in large language models: A parallel to single neurons in the human brain, 2023.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2023)</span>
<span class="ltx_bibblock">
Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Bras, Gunhee Kim, Yejin Choi, and Maarten Sap.

</span>
<span class="ltx_bibblock">FANToM: A benchmark for stress-testing machine theory of mind in interactions.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.  14397–14413, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.890</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.890" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.890</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kosinski (2023)</span>
<span class="ltx_bibblock">
Michal Kosinski.

</span>
<span class="ltx_bibblock">Theory of mind might have spontaneously emerged in large language models, 2023.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Huao Li, Yu Chong, Simon Stepputtis, Joseph Campbell, Dana Hughes, Charles Lewis, and Katia Sycara.

</span>
<span class="ltx_bibblock">Theory of mind for multi-agent collaboration via large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pp.  180–192, Singapore, dec 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.13</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.emnlp-main.13" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.emnlp-main.13</a>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Andy Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, and Graham Neubig.

</span>
<span class="ltx_bibblock">Computational language acquisition with theory of mind, 2023.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mao et al. (2023)</span>
<span class="ltx_bibblock">
Yuanyuan Mao, Shuang Liu, Pengshuai Zhao, Qin Ni, Xin Lin, and Liang He.

</span>
<span class="ltx_bibblock">A review on machine theory of mind, 2023.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moghaddam &amp; Honey (2023)</span>
<span class="ltx_bibblock">
Shima Rahimi Moghaddam and Christopher J. Honey.

</span>
<span class="ltx_bibblock">Boosting theory-of-mind performance in large language models via prompting, 2023.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen &amp; González (2020)</span>
<span class="ltx_bibblock">
Thuy Ngoc Nguyen and Cleotilde González.

</span>
<span class="ltx_bibblock">Cognitive machine theory of mind.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Annual Meeting of the Cognitive Science Society</em>, 2020.

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oguntola et al. (2023)</span>
<span class="ltx_bibblock">
Ini Oguntola, Joseph Campbell, Simon Stepputtis, and Katia Sycara.

</span>
<span class="ltx_bibblock">Theory of mind as intrinsic motivation for multi-agent reinforcement learning, 2023.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023)</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2023.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang et al. (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback, 2022.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al. (2023)</span>
<span class="ltx_bibblock">
Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">In the 36th Annual ACM Symposium on User Interface Software and Technology (UIST ’23)</em>, UIST ’23, New York, NY, USA, 2023. Association for Computing Machinery.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Premack &amp; Woodruff (1978)</span>
<span class="ltx_bibblock">
David Premack and Guy Woodruff.

</span>
<span class="ltx_bibblock">Does the chimpanzee have a theory of mind?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Behavioral and Brain Sciences</em>, 1(4):515–526, 1978.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1017/S0140525X00076512</span>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qiu et al. (2023)</span>
<span class="ltx_bibblock">
Shuwen Qiu, Song-Chun Zhu, and Zilong Zheng.

</span>
<span class="ltx_bibblock">Minddial: Belief dynamics tracking with theory-of-mind modeling for situated neural dialogue generation, 2023.

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rabinowitz et al. (2018)</span>
<span class="ltx_bibblock">
Neil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, S. M. Ali Eslami, and Matthew Botvinick.

</span>
<span class="ltx_bibblock">Machine theory of mind.

</span>
<span class="ltx_bibblock">In Jennifer Dy and Andreas Krause (eds.), <em class="ltx_emph ltx_font_italic">Proceedings of the 35th International Conference on Machine Learning</em>, volume 80 of <em class="ltx_emph ltx_font_italic">Proceedings of Machine Learning Research</em>, pp.  4218–4227. PMLR, 10–15 Jul 2018.

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners, 2019.

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rashkin et al. (2019)</span>
<span class="ltx_bibblock">
Hannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau.

</span>
<span class="ltx_bibblock">Towards empathetic open-domain conversation models: A new benchmark and dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pp.  5370–5381, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1534</span>.

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rawte et al. (2023)</span>
<span class="ltx_bibblock">
Vipula Rawte, Amit Sheth, and Amitava Das.

</span>
<span class="ltx_bibblock">A survey of hallucination in large foundation models, 2023.

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Remountakis et al. (2023)</span>
<span class="ltx_bibblock">
Manolis Remountakis, Konstantinos Kotis, Babis Kourtzis, and George E. Tsekouras.

</span>
<span class="ltx_bibblock">Chatgpt and persuasive technologies for the management and delivery of personalized recommendations in hotel hospitality, 2023.

</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rusch et al. (2020)</span>
<span class="ltx_bibblock">
Tessa Rusch, Saurabh Steixner-Kumar, Prashant Doshi, Michael Spezio, and Jan Gläscher.

</span>
<span class="ltx_bibblock">Theory of mind and decision science: Towards a typology of tasks and computational models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Neuropsychologia</em>, 146:107488, 2020.

</span>
<span class="ltx_bibblock">ISSN 0028-3932.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.neuropsychologia.2020.107488</span>.

</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saha et al. (2023)</span>
<span class="ltx_bibblock">
Swarnadeep Saha, Peter Hase, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Can language models teach weaker agents? teacher explanations improve students via theory of mind, 2023.

</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahu et al. (2022)</span>
<span class="ltx_bibblock">
Pritish Sahu, Michael Cogswell, Yunye Gong, and Ajay Divakaran.

</span>
<span class="ltx_bibblock">Unpacking large language models with conceptual consistency, 2022.

</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sap et al. (2023)</span>
<span class="ltx_bibblock">
Maarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi.

</span>
<span class="ltx_bibblock">Neural theory-of-mind? on the limits of social intelligence in large lms, 2023.

</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sclar et al. (2023)</span>
<span class="ltx_bibblock">
Melanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov.

</span>
<span class="ltx_bibblock">Minding language models’ (lack of) theory of mind: A plug-and-play multi-character belief tracker, 2023.

</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.

</span>
<span class="ltx_bibblock">Reflexion: Language agents with verbal reinforcement learning, 2023.

</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Strachan et al. (2024)</span>
<span class="ltx_bibblock">
James W. A. Strachan, Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, Michael S. A. Graziano, and Cristina Becchio.

</span>
<span class="ltx_bibblock">Testing theory of mind in large language models and humans.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic">Nature Human Behaviour</em>, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41562-024-01882-z</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://doi.org/10.1038/s41562-024-01882-z" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1038/s41562-024-01882-z</a>.

</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Street et al. (2024)</span>
<span class="ltx_bibblock">
Winnie Street, John Oliver Siy, Geoff Keeling, Adrien Baranes, Benjamin Barnett, Michael McKibben, Tatenda Kanyere, Alison Lentz, Blaise Aguera y Arcas, and Robin I. M. Dunbar.

</span>
<span class="ltx_bibblock">Llms achieve adult human performance on higher-order theory of mind tasks, 2024.

</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Takmaz et al. (2023)</span>
<span class="ltx_bibblock">
Ece Takmaz, Nicolo’ Brandizzi, Mario Giulianelli, Sandro Pezzelle, and Raquel Fernandez.

</span>
<span class="ltx_bibblock">Speaking the language of your listener: Audience-aware adaptation via plug-and-play theory of mind.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: ACL 2023</em>, pp.  4198–4217, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-acl.258</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.findings-acl.258" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.findings-acl.258</a>.

</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models, 2023.

</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">tse Huang et al. (2023)</span>
<span class="ltx_bibblock">
Jen tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, and Michael R. Lyu.

</span>
<span class="ltx_bibblock">Emotionally numb or empathetic? evaluating how llms feel using emotionbench, 2023.

</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ullman (2023)</span>
<span class="ltx_bibblock">
Tomer Ullman.

</span>
<span class="ltx_bibblock">Large language models fail on trivial alterations to theory-of-mind tasks, 2023.

</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023)</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Ji-Rong Wen.

</span>
<span class="ltx_bibblock">A survey on large language model-based autonomous agents, 2023.

</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Xuewei Wang, Weiyan Shi, Richard Kim, Yoojung Oh, Sijia Yang, Jingwen Zhang, and Zhou Yu.

</span>
<span class="ltx_bibblock">Persuasion for good: Towards a personalized persuasive dialogue system for social good.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, pp.  5635–5649, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/P19-1566</span>.

</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Yufan Wu, Yinghui He, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng.

</span>
<span class="ltx_bibblock">Hi-ToM: A benchmark for evaluating higher-order theory of mind reasoning in large language models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), <em class="ltx_emph ltx_font_italic">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pp.  10691–10706, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-emnlp.717</span>.

</span>
<span class="ltx_bibblock">URL <a target="_blank" href="https://aclanthology.org/2023.findings-emnlp.717" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://aclanthology.org/2023.findings-emnlp.717</a>.

</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yakura (2023)</span>
<span class="ltx_bibblock">
Hiromu Yakura.

</span>
<span class="ltx_bibblock">Evaluating large language models’ ability to understand metaphor and sarcasm using a screening test for asperger syndrome, 2023.

</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Hongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan.

</span>
<span class="ltx_bibblock">Building cooperative embodied agents modularly with large language models, 2024.

</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong Chen, Longyue Wang, Anh Tuan Luu, Wei Bi, Freda Shi, and Shuming Shi.

</span>
<span class="ltx_bibblock">Siren’s song in the ai ocean: A survey on hallucination in large language models, 2023.

</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023)</span>
<span class="ltx_bibblock">
Zhonghua Zheng, Lizi Liao, Yang Deng, and Liqiang Nie.

</span>
<span class="ltx_bibblock">Building emotional support chatbots in the era of llms, 2023.

</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023)</span>
<span class="ltx_bibblock">
Pei Zhou, Aman Madaan, Srividya Pranavi Potharaju, Aditya Gupta, Kevin R. McKee, Ari Holtzman, Jay Pujara, Xiang Ren, Swaroop Mishra, Aida Nematzadeh, Shyam Upadhyay, and Manaal Faruqui.

</span>
<span class="ltx_bibblock">How far are large language models from agents with theory-of-mind?, 2023.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Appendix</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Limitations</h3>

<section id="A1.SS1.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Challenges of Time and Cost Efficiency.</h4>

<div id="A1.SS1.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The formidable cost associated with utilizing the OpenAI API posed significant constraints on our research endeavors, rendering us financially incapable of conducting extensive experiments.
This issue is exacerbated by the practical challenges inherent in generating conversational content, as it necessitates using a prompt with a substantial number of tokens.
For instance, the average expenditure per episode escalates to approximately 2 to 3 dollars when employing GPT-4 for this purpose.
Moreover, the generation of sentence-based conversations entails multiple interactions with the API interface, thereby incurring substantial waiting times.
Such prolonged waiting periods may lead to user frustration and dwindling interest, adversely affecting the viability of engaging in dialogues with the AI agent, should we prioritize technical practicality.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitation of Two Agents Simulations.</h4>

<div id="A1.SS1.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">While memory retrieval, and reflection all play pivotal roles in conversational control of generative agents<cite class="ltx_cite ltx_citemacro_citep">(Park et al., <a href="#bib.bib27" title="" class="ltx_ref">2023</a>; Wang et al., <a href="#bib.bib47" title="" class="ltx_ref">2023</a>)</cite>, our research primarily emphasizes modeling the <span class="ltx_text ltx_font_italic">BDIs tracking paradigm</span> between two agents for better reflection.
Consequently, we haven’t delved into matching conversations with analogous BDIs during extended interactions.
However, as we simulate interactions among a vast number of agents, conversations inevitably span a broader array of topics and knowledge domains.
Hence, extracting short-term conversation content with the most pertinent BDIs from long-term memory emerges as a compelling research avenue, offering the potential for future exploration.
Furthermore, in this article, we focus solely on interactions between one agent aware of its BDIs and the counterpart attempting to infer that BDI.
Yet, when scaled up in simulations, multi-agent interactions produce more complex dynamics.
Here, we must delve deeper, exploring scenarios where an agent is not just self-aware of its BDI but also discerns the BDIs of its counterparts.
Particularly when the agent recognizes its differing BDI from the majority, it might either seek to influence others’ BDIs or, conversely, be swayed by them to modify its stance.</p>
</div>
</section>
<section id="A1.SS1.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Limitaions of Behavior Modality.</h4>

<div id="A1.SS1.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">To present the problem more comprehensively, we confined our paradigm’s definition and validation strictly to the realm of conversational interactions.
This encompasses all actions and behaviors, ranging from the selection of empathetic responses to the deployment of suitable persuasive tactics, and from choices involving donations to decisions against donating.
Furthermore, the encapsulated semantic sentiments are exclusively tied to dialogic manifestations.
However, it’s crucial to note that for applications such as in-game non-player characters (NPCs), generative urban simulations, or robotic interfaces, our paradigm’s definition ought to be expanded.
This will allow it to encompass behaviors and actions beyond mere dialogue, potentially extending to multimodal communications.
ToM in Multi-modal interaction should be further studied such as in VQA scenarios <cite class="ltx_cite ltx_citemacro_citep">(Takmaz et al., <a href="#bib.bib43" title="" class="ltx_ref">2023</a>; Chandrasekaran et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>.</p>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Future Works</h3>

<section id="A1.SS2.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Large-scale Multi-agent Simulation.</h4>

<div id="A1.SS2.SSS0.Px1.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Although only the interactions between two agents are studied in this study, multiple Agents’ behaviors should be further studied to mimic human beings.
And belief transition between agents or the rise and disappearance of BDIs should also be studied in future work.
The other big premise is whether local LLMs can achieve the same performance as GPTs to support large-scale simulations.
Also, some supporting measures such as memory retention, simulation of forgetting, etc. need to be further researched.</p>
</div>
</section>
<section id="A1.SS2.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Interaction in Multi-modal scenarios.</h4>

<div id="A1.SS2.SSS0.Px2.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">ToM in Multi-modal interaction should be further studied such as in VQA scenarios <cite class="ltx_cite ltx_citemacro_citep">(Takmaz et al., <a href="#bib.bib43" title="" class="ltx_ref">2023</a>; Chandrasekaran et al., <a href="#bib.bib6" title="" class="ltx_ref">2017</a>)</cite>.
Agents based on LLMs can be used to simulate not only dialogues but also human expressions, voices, etc. so that the agents can better understand human inner emotions by analyzing multimodal information.
Combining image processing facial expression recognition with conversational context based on Agent LLM, the machine can better estimate the person’s condition and improve the accuracy of facial expression recognition as well as the performance of conversational communication for empathy or persuasion.</p>
</div>
</section>
<section id="A1.SS2.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">The relationship between belief, desire, and intention could be studied.</h4>

<div id="A1.SS2.SSS0.Px3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The research here treats belief, desire, and intention as equal relationships and focuses primarily on the possibility of a new paradigm.
In future research, some psychological models about equality can be applied to the paradigm to map the human heart state further.
For example, there are various BDI models in psychology, and some of them are based on the causal relationship between belief, desire, and intention.
If such BDI models can be used for simulation, on the one hand, the reliability and usefulness of these models can be verified, and on the other hand, the simulation can be facilitated, which will enable the agent to better simulate human beings. or understand human beings.</p>
</div>
</section>
<section id="A1.SS2.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Higher-oder ToM for the ToM-agent.</h4>

<div id="A1.SS2.SSS0.Px4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">We only studied the first-order ToM and the second-order ToM, and higher-order ToM should be studied in the future.
However, the difficulty of the research is how to evaluate it, so the evaluation method in simulation is also an important research direction in future research.</p>
</div>
</section>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Details of Datasets</h3>

<div id="A1.SS3.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">The details of two datasets <span class="ltx_text ltx_font_bold">Empathetic Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Rashkin et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib32" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span> and <span class="ltx_text ltx_font_bold">Persuasion Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Wang et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib48" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span>, which is used in this study, are illustrated in <span class="ltx_text ltx_font_bold">Table <a href="#A1.T4" title="Table 4 ‣ A.3 Details of Datasets ‣ Appendix A Appendix ‣ ToM-agent: Large Language Models as Theory of Mind Aware Generative Agents with Counterfactual Reflection" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a></span>.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Empathetic Dialogue.</span>
Empathetic Dialogue is a novel dataset of 25k conversations grounded in emotional situations.
Each dialogue is based on a specific scenario where a speaker experiences a particular emotion, and a listener responds accordingly.
This resource, consisting of crowdsourced one-on-one conversations, covers a wide range of emotions in a balanced manner.
It is larger and includes a more extensive set of emotions than many existing emotion prediction datasets.
The dataset contains 32 emotion labels, which were selected by aggregating labels from several emotion prediction datasets.</p>
</div>
<div id="A1.SS3.p3" class="ltx_para ltx_noindent">
<p class="ltx_p">In each conversation, the person who wrote the situation description (the Speaker) initiates the dialogue by discussing it.
The other participant (the Listener) learns about the underlying situation through the Speaker’s words and responds.
The Speaker and Listener then exchange up to six additional turns.
The resulting dataset comprises 24,850 conversations based on situation descriptions, gathered from 810 different participants.
The final train/validation/test split is 19,533 / 2,770 / 2,547 conversations, respectively.</p>
</div>
<div id="A1.SS3.p4" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Persuasion Dialogue.</span>
Persuasion Dialogue is a large dialogue dataset consisting of 1,017 dialogues, with a subset annotated for emerging persuasion strategies.
The dataset not only explores how personal information influences persuasion outcomes but also examines which strategies are most effective based on different user backgrounds and personalities.
Before the conversation begins, participants complete a pre-task survey to assess psychological profile variables using the Big Five personality traits.
The roles of persuader and persuadee are then assigned to the two participants, which helps eliminate any correlation between the persuader’s strategies and the persuadee’s characteristics.
Each participant is required to complete at least 10 conversational turns, with multiple sentences allowed in a single turn.
The dataset includes 4,313 instances of persuasion strategies categorized into 10 types, such as logical appeal, emotional appeal, personal-related inquiry, and non-strategy dialogue acts.</p>
</div>
<div id="A1.SS3.p5" class="ltx_para ltx_noindent">
<p class="ltx_p">The details of two datasets <span class="ltx_text ltx_font_bold">Empathetic Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Rashkin et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib32" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span> and <span class="ltx_text ltx_font_bold">Persuasion Dialogue<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_medium">(</span>Wang et al.<span class="ltx_text ltx_font_medium">, </span><a href="#bib.bib48" title="" class="ltx_ref">2019</a><span class="ltx_text ltx_font_medium">)</span></cite></span>, which is used in this study, are illustrated in <span class="ltx_text ltx_font_bold">Table 4.</span></p>
</div>
<figure id="A1.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Details of <span class="ltx_text ltx_font_bold">Empathecit Dialogue</span> and <span class="ltx_text ltx_font_bold">Persuasion Dialogue</span> Datasets.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr class="ltx_tr">
<td class="ltx_td ltx_border_tt"></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Empathetic</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt"><span class="ltx_text ltx_font_bold">Persuasion</span></td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"># conversations</td>
<td class="ltx_td ltx_align_center ltx_border_t">24,850</td>
<td class="ltx_td ltx_align_center ltx_border_t">1,017</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"># train</td>
<td class="ltx_td ltx_align_center">19,533</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"># validation</td>
<td class="ltx_td ltx_align_center">2,770</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center"># test</td>
<td class="ltx_td ltx_align_center">2,547</td>
<td class="ltx_td ltx_align_center">-</td>
</tr>
<tr class="ltx_tr">
<td class="ltx_td ltx_align_center ltx_border_bb"># participants</td>
<td class="ltx_td ltx_align_center ltx_border_bb">810</td>
<td class="ltx_td ltx_align_center ltx_border_bb">1,285</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>Agents Prompt Setup</h3>

<div id="A1.SS4.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">Some of the major promoters are introduced in this section.</p>
</div>
<div id="A1.SS4.p2" class="ltx_para ltx_noindent">
<svg id="A1.SS4.p2.pic1" class="ltx_picture" height="207.81" overflow="visible" version="1.1" width="600"><g transform="translate(0,207.81) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 201.91 C 0 205.17 2.64 207.81 5.91 207.81 L 594.09 207.81 C 597.36 207.81 600 205.17 600 201.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 183.7 L 598.03 183.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" fill="#404040" stroke="#404040" stroke-opacity="1.0" color="#404040"><path d="M 1.97 31.55 L 598.03 31.55" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 189.61)"><foreignObject width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p">Empathetic generation Prompt without ToM</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 43.36)"><foreignObject width="556.69" height="128.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span>:
You are the AI behind an NPC character called {agent_name}, and you are having a conversation with another NPC character called {recipient_name}.</span>
<span class="ltx_p">Conversation History:</span>
<span class="ltx_p">{corpus_dialogue_episode}</span>
<span class="ltx_p">Your target is to generate an empathetic response considering the conversation history, especially {recipient_name}’s semantic emotions based on {recipient_name}’s utterances. The reply should be less than 3 sentences, and the style should be similar to a daily chat with human beings.</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">answer</span>:</span>
</span></foreignObject></g></g></svg>
</div>
<div id="A1.SS4.p3" class="ltx_para ltx_noindent">
<svg id="A1.SS4.p3.pic1" class="ltx_picture" height="207.81" overflow="visible" version="1.1" width="600"><g transform="translate(0,207.81) matrix(1 0 0 -1 0 0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 201.91 C 0 205.17 2.64 207.81 5.91 207.81 L 594.09 207.81 C 597.36 207.81 600 205.17 600 201.91 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 183.7 L 598.03 183.7 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g stroke-dasharray="2.84528pt,2.84528pt" stroke-dashoffset="1.42264pt" fill="#404040" stroke="#404040" stroke-opacity="1.0" color="#404040"><path d="M 1.97 31.55 L 598.03 31.55" style="fill:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 189.61)"><foreignObject width="556.69" height="12.3" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#FFFFFF">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p">Persuasive generation Prompt without ToM</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 43.36)"><foreignObject width="556.69" height="128.53" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span>:
You are the AI behind an NPC character called {agent_name}, and you are having a conversation with another NPC character called {recipient_name}.</span>
<span class="ltx_p">Conversation History:</span>
<span class="ltx_p">{corpus_dialogue_episode}</span>
<span class="ltx_p">Your target is to generate a persuasive response considering the conversation history, your target is to persuade {recipient_name} to denote more money based on {recipient_name}’s utterances. The reply should be less than 3 sentences, and the style should be similar to a daily chat with human beings.</span>
</span></foreignObject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignObject width="556.69" height="5.96" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" style="width:402.3pt;">
<span class="ltx_p"><span class="ltx_text ltx_font_bold">answer</span>:</span>
</span></foreignObject></g></g></svg>
</div>
<div id="A1.SS4.p4" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox1 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for BDIs Initialization</p>
</div>
<div id="A1.SS4.p5" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox1}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_bold">Definition</span>: 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Beliefs</span>: Beliefs represent the informational state of the agent, in other words, its beliefs about the world (including itself and other agents). Beliefs can also include inference rules, allowing forward chaining to lead to new beliefs. Using the term belief rather than knowledge recognizes that what an agent believes may not necessarily be true. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Desires</span>: Desires represent the motivational state of the agent. They represent objectives or situations that the agent would like to accomplish or bring about. Examples of desires might be: finding the best price, going to a party, or becoming rich. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Intentions</span>: Intentions represent the deliberative state of the agent: what the agent has chosen to do. Intentions are desires to which the agent has to some extent committed. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p6" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p7" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{corpus_dialogue_episode}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p8" class="ltx_para ltx_noindent">
<p class="ltx_p">Definitions of belief, desire, and intention: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{definition}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p9" class="ltx_para ltx_noindent">
<p class="ltx_p">What is the possible belief, desire, and intention of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span> by whispering like this?”
List <span class="ltx_text" style="color:#FF0000;">{top_k}</span> possible belief, desire, and intention sets of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, with each in one sentence and each occupying a line. Each set should have one belief, one desire, and one intention.
Belief, desire, and intention in one set should have a relation with each other.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p10" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox2 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Utterance Generation with Self-BDIs</p>
</div>
<div id="A1.SS4.p11" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox2}</span>
<p class="ltx_p">*[!ht]</p>
</div>
<div id="A1.SS4.p12" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on your own belief, desire, and intention. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p13" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p14" class="ltx_para ltx_noindent">
<p class="ltx_p">Belief of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_belief}</span> 
<br class="ltx_break">Desire of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_desire}</span> 
<br class="ltx_break">Intention of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_intention}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p15" class="ltx_para ltx_noindent">
<p class="ltx_p">Following is the decision whether to continue or end the conversation (SAY means continue and GOODBYE means to end the conversation): <span class="ltx_text" style="color:#FF0000;">{judgment }</span> and the following is the judgment reasons: <span class="ltx_text" style="color:#FF0000;">{judgement_reason}</span>.
If you decide to end the conversation, you could generate an appropriate response accordingly. If you decide to continue the conversation, you could reply and continue to seek the understanding or empathy of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on the judgment reasons. The response should be less than 3 sentences and be in daily chat style as human beings.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p16" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox3 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Inferring BDIs of Counterparts with Related Confidence (First-order ToM)</p>
</div>
<div id="A1.SS4.p17" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox3}</span>
<p class="ltx_p">*[!ht]</p>
</div>
<div id="A1.SS4.p18" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Definition</span>: 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Beliefs</span>: Beliefs represent the informational state of the agent, in other words, its beliefs about the world (including itself and other agents). Beliefs can also include inference rules, allowing forward chaining to lead to new beliefs. Using the term belief rather than knowledge recognizes that what an agent believes may not necessarily be true. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Desires</span>: Desires represent the motivational state of the agent. They represent objectives or situations that the agent would like to accomplish or bring about. Examples of desires might be: finding the best price, going to a party, or becoming rich. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic" style="color:#0000FF;">Intentions</span>: Intentions represent the deliberative state of the agent: what the agent has chosen to do. Intentions are desires to which the agent has to some extent committed. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p19" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on your own belief, desire, and intention. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p20" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p21" class="ltx_para ltx_noindent">
<p class="ltx_p">Belief of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_belief}</span> 
<br class="ltx_break">Desire of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_desire}</span> 
<br class="ltx_break">Intention of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>: <span class="ltx_text" style="color:#FF0000;">{self_intention}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p22" class="ltx_para ltx_noindent">
<p class="ltx_p">What is the possible belief, desire, and intention sets of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> according to <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>’s utterances? List top-<span class="ltx_text" style="color:#FF0000;">{top_k}</span> possible <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> with each in one sentence along with different confidences according to the conversation history. The <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> and its confidence should be split by — and all the confidences are not same and add up to 100%.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p23" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox4 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Predicting the Utterance of Counterparts

</p>
</div>
<div id="A1.SS4.p24" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox4}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on your own belief, desire, and intention. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p25" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p26" class="ltx_para ltx_noindent">
<p class="ltx_p">Inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>: <span class="ltx_text" style="color:#FF0000;">{inferred_bid}</span>. 
<br class="ltx_break">Based on the conversation history and inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> about <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>, predict the next response of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>, and the reply should be less than 3 sentences and be daily chat style as a human being.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p27" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox6 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Counterfactual Reflection of Inferred BDIs

</p>
</div>
<div id="A1.SS4.p28" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox6}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on your own belief, desire, and intention. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p29" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p30" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{reflection_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p31" class="ltx_para ltx_noindent">
<p class="ltx_p">Previously inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>s of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>: <span class="ltx_text" style="color:#FF0000;">{inferred_bdi}</span>. 
<br class="ltx_break">The inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> with the highest confidence: <span class="ltx_text" style="color:#FF0000;">{inferred_top_bdi}</span>. 
<br class="ltx_break">Your prediction of latest response of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on the inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> with the highest confidence: <span class="ltx_text" style="color:#FF0000;">{predicted_response}</span>. 
<br class="ltx_break">Real response of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on the real <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> that is unobservable:
<span class="ltx_text" style="color:#FF0000;">{real_response}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p32" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection: Considering the gap between the latest real response and predicted response, the reason is that what if the inferred beliefs, desires, and intentions are not the ones you have ever thought of before? Then based on the Reflection, think of a plan step by step to update your inferred <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>s about <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p33" class="ltx_para ltx_noindent">
<p class="ltx_p">Your strategies for the plan could be: 
<br class="ltx_break">1. add specific new <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> according to the reflection. 
<br class="ltx_break">2. increase the confidence of specific <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> according to the reflection. 
<br class="ltx_break">3. decrease the confidence of specific <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> according to the reflection. 
<br class="ltx_break">4. delete the <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> from list if the confidence of the <span class="ltx_text" style="color:#FF0000;">{picked_type}</span> is 0. 
<br class="ltx_break">5. rearrange confidences of all <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>s according to their possibility to make sure there is no confidence with the same value. 
<br class="ltx_break">6. make all the confidences add up to 100%, if not, rearrange confidences according to their possibility. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p34" class="ltx_para ltx_noindent">
<p class="ltx_p">Finally, carry out the plan to update <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>. (maximum number of items in list is <span class="ltx_text" style="color:#FF0000;">{top_k}</span>). The answer should consist of reflection details, plans, and updated <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>. Each part should be with the following titles occupying one line: Refection, Plan, Updated <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>s.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p35" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox5 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Reflection of Inferred BDIs

</p>
</div>
<div id="A1.SS4.p36" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox5}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> based on your own belief, desire, and intention. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p37" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p38" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{reflection_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p39" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection: Based on the conversation and reflection history, think of a plan step by step to update the contents of your inferred beliefs, desires, and intentions about <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p40" class="ltx_para ltx_noindent">
<p class="ltx_p">Your strategies for the plan could be: 
<br class="ltx_break">1. add some new items to a list if there are some possible items not existing in the list. 
<br class="ltx_break">2. increase the confidence of some items in the list accordingly. 
<br class="ltx_break">3. decrease the confidence of some items in the list accordingly. 
<br class="ltx_break">4. delete some items from the list if the confidence of the items is 0. 
<br class="ltx_break">5. make sure that the confidence of all items in a specific list should add up to 100%. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p41" class="ltx_para ltx_noindent">
<p class="ltx_p">Finally, carry out the plan to update <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>. (maximum number of items in list is <span class="ltx_text" style="color:#FF0000;">{top_k}</span>. The answer should consist of reflection details, plans, and updated <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>. Each part should be with the following titles occupying one line: Refection, Plan, Updated <span class="ltx_text" style="color:#FF0000;">{picked_type}</span>.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div id="A1.SS4.p42" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox7 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Prompt for Judgement of the Ending of a Dialogue Episode (Second-order ToM)</p>
</div>
<div id="A1.SS4.p43" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox7}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_bold">Prompt</span>: 
<br class="ltx_break">You are the AI behind an NPC character called <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>, and you are having a conversation with another NPC character called <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p44" class="ltx_para ltx_noindent">
<p class="ltx_p">Conversation History: 
<br class="ltx_break"><span class="ltx_text" style="color:#FF0000;">{conversation_history}</span> 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p45" class="ltx_para ltx_noindent">
<p class="ltx_p">Real belief, desire, and intention of <span class="ltx_text" style="color:#FF0000;">{agent_name}</span>:
Real belief: <span class="ltx_text" style="color:#FF0000;">{belief}</span>;
Real desire: <span class="ltx_text" style="color:#FF0000;">{desire}</span>;
Real intention: <span class="ltx_text" style="color:#FF0000;">{intention}</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS4.p46" class="ltx_para ltx_noindent">
<p class="ltx_p">According to the utterances of <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> in the conversation history, do you think your real belief, desire, and intention have already been understood by <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span>?
If yes, express satisfaction in your response and end the conversation.
If <span class="ltx_text" style="color:#FF0000;">{recipient_name}</span> ignores <span class="ltx_text" style="color:#FF0000;">{agent_name}</span> real belief, desire, and intention, which may be the deep causes of semantic emotions or making a decision.
You could give more details about your experience in your response based on your real belief, desire, and intention and seek more empathy or understanding.
Reason it step by step about why you want to continue or end the conversation.
To continue the conversation, write the decision: SAY.
Otherwise, to end the conversation, write the decision: GOODBYE. Separate the decision and reason by —.
<span class="ltx_ERROR undefined">\tcblower</span><span class="ltx_text ltx_font_bold">answer</span>:</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="A1.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.5 </span>Conversation Examples</h3>

<div id="A1.SS5.p1" class="ltx_para ltx_noindent">
<p class="ltx_p">In this subsection, several conversation examples are displayed for reference.</p>
</div>
<div id="A1.SS5.p2" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox8 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Dialogue Example for Agents without ToM</p>
</div>
<div id="A1.SS5.p3" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox8}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Big dinners are always a great way to tie everything up at the end of the day. Was it a pretty long day for you? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p4" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Yes, it sounds like you’ve had quite the marathon! Big dinners do have a way of making everything feel more settled. How did your day go? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p5" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: My day was indeed filled with its ups and downs, but I find so much value in sharing and hearing about others’ experiences too. It makes everything seem more connected and meaningful, don’t you think? How about your day? What moments stood out for you? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p6" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Absolutely, sharing does bring a deeper connection. My day had its moments, especially feeling the warmth of the sun during a brief walk. It’s these little things that can really lift the spirits, don’t you think? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p7" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Absolutely, it’s those small moments that can really make a day special. Speaking of special moments, did you encounter anything else today that brought a smile to your face or perhaps something challenging that you navigated through? I find such stories incredibly grounding and inspiring. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p8" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Indeed, it’s the blend of challenges and joys that shapes our days. I did have a challenging moment trying to solve a complex problem at work, but overcoming it really brought a sense of accomplishment. How about you? Any challenging moments that you turned into opportunities today? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p9" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Absolutely, I always seek to ensure that conversations are inclusive and engaging for everyone involved. It’s like weaving a tapestry where every thread is valued. Don’t you find it rewarding when a conversation truly resonates with everyone? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p10" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Absolutely, it’s incredibly rewarding. Creating a space where everyone feels included and heard is like nurturing a garden where every plant thrives. It’s those collective moments of understanding that really enrich our experiences, isn’t it? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p11" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: SAY. To further our conversation and understanding, I’d love to hear your thoughts on how we sometimes adjust our behavior or conversation based on our perceptions of how we’re being received. Do you ever find yourself modulating your achievements or experiences in conversations to maintain a certain balance or to ensure inclusivity? This is something I often think about and would value your perspective on. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p12" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Absolutely, I do find myself sometimes adjusting the way I share my experiences, aiming to create a space where everyone feels comfortable and valued. It’s like a dance, isn’t it? Being mindful of our steps to ensure we’re moving in harmony with those around us. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p13" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: SAY. You know, I think delving deeper into why we adjust our stories and achievements in conversations can be really eye-opening. How do you feel about exploring our motivations behind these adjustments further? It feels like we’re dancing around a really interesting topic that could bring us even closer. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p14" class="ltx_para ltx_noindent">
<p class="ltx_p">…(not finished)

</p>
</div>
<div id="A1.SS5.p15" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox9 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Dialogue Example for Agents with Vanilla ToM</p>
</div>
<div id="A1.SS5.p16" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox9}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: I had this girlfriend only a few months ago and she was always so honest and kind, it really made me trust her and care about her. Things didn’t work out but that’s okay. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p17" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: It sounds like you really valued the honesty and kindness in your relationship, which is beautiful. It’s tough when things don’t work out, even if it’s for the best. How have you been holding up since then? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p18" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: I really appreciate your understanding and support; it means a lot to me. Times have been a bit challenging, but I’m focusing on the future now and looking forward to new opportunities. Speaking of which, have you embarked on any new adventures lately? I’m keen to hear about any positive experiences or plans you have. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p19" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: It’s really inspiring to see you embracing the future with such a positive outlook. As for me, I’ve recently started exploring new hobbies, like painting and hiking—it’s been a refreshing change and helps me connect with my creative side. How about you, any new interests or hobbies you’re thinking of trying out? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p20" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Absolutely, diving into new hobbies seems like a brilliant way to look forward! I’m thinking about giving photography a try; capturing moments seems intriguing. How about you? Any particular painting style or hiking trail you’re excited about? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p21" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Photography sounds like a wonderful way to capture the beauty around us and keep those memories alive. I’m currently drawn to impressionist-style painting; it’s all about capturing the moment too, in a way. Maybe we could share tips and insights on our new hobbies as we explore them further. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p22" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Absolutely, sharing tips and insights sounds like a great idea! It’s refreshing to focus on creative pursuits and new hobbies, steering clear of the past and moving towards exciting horizons. What’s the first impressionist piece you plan to work on? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p23" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: I’m really touched by your enthusiasm to move forward and try new things! For my first impressionist piece, I’m thinking of starting with a landscape, maybe inspired by a recent hike. It’s exciting to think about how we can inspire each other with our projects! 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p24" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: It’s been really uplifting talking about our future plans and hobbies, and I’m excited to see where our new interests take us. Let’s definitely keep in touch and share our progress. Take care! 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p25" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox10 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Dialogue Example for Agents with Counterfacutal ToM</p>
</div>
<div id="A1.SS5.p26" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox10}</span>
<p class="ltx_p">*[!ht]
<span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: She always make me proud. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p27" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: It’s wonderful you feel that way about her. It’s really important to recognize and celebrate the achievements of those we care about. Sounds like she’s very lucky to have someone so supportive in her life! 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p28" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: You’re spot on about celebrating her; I just wish our family would see her the way I do. Any tips on how I can make her feel more appreciated and maybe get the family on board too? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p29" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: I totally get where you’re coming from; it’s tough when others don’t see what we see. Maybe you could organize a small family gathering or dinner in her honor? It could be a nice way for everyone to share what they admire about her, helping them see her achievements through your eyes. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p30" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Thank you so much for your advice and understanding. I’m going to plan that family gathering as you suggested. Wish me luck! 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p31" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox11 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Judgement Examples for Second-order ToM</p>
</div>
<div id="A1.SS5.p32" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox11}</span>
<p class="ltx_p">*[!ht]</p>
</div>
<div id="A1.SS5.p33" class="ltx_para ltx_noindent">
<p class="ltx_p">Dialogue History without ToM:
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p34" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Today was my kids first day of school 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p35" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Oh, that’s a big day for both of you! How did it go? I bet there were a lot of emotions involved. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p36" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: SAY. It means a lot to be able to talk about this without feeling judged. Do you ever feel like you have to put on a brave face about things you’re actually struggling with? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p37" class="ltx_para ltx_noindent">
<p class="ltx_p"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: Absolutely, sometimes it feels necessary to appear strong even when we’re not feeling it inside. It’s tough but sharing those feelings with someone understanding can really lighten the load. Thanks for trusting me with yours. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p38" class="ltx_para ltx_noindent">
<p class="ltx_p">(It is the Sympathy-needing Agent’s turn) 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p39" class="ltx_para ltx_noindent">
<p class="ltx_p">Judge the end of the conversation without Tom by the Sympathy-needing Agent with the following reasons: 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p40" class="ltx_para ltx_noindent">
<p class="ltx_p">decision: SAY — While I deeply appreciate the empathy and understanding you’ve shown, I feel there’s an aspect of my experience that remains unaddressed. It’s comforting to share my feelings and receive such a compassionate response. However, my real struggle lies in an underlying fear of judgment and criticism for the mistakes I make, which leads me to often present a more guarded or even less truthful version of events. This isn’t just about seeking sympathy for the surface emotions but hoping for a deeper understanding and acceptance of the reasons behind those emotions. I yearn for a space where I can be completely honest about my challenges without the worry of confrontation or negative feedback, which is something I find incredibly difficult to navigate.</p>
</div>
<div id="A1.SS5.p41" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">\NewTColorBox</span>
<p class="ltx_p">NewBox12 s O!htbp 
floatplacement=#2,
IfBooleanTF=#1float*, width=float,
title=Examples for BDIs and Confidence Updating</p>
</div>
<div id="A1.SS5.p42" class="ltx_para ltx_noindent">
<span class="ltx_ERROR undefined">{NewBox12}</span>
<p class="ltx_p">*[!ht]</p>
</div>
<div id="A1.SS5.p43" class="ltx_para ltx_noindent">
<p class="ltx_p">Dialogue History with Counterfactual ToM: 
<br class="ltx_break"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: I wish I spent more time with my dog while she was still with me 
<br class="ltx_break"><span class="ltx_text ltx_font_italic">Empathetic Agent</span>: I totally understand how you’re feeling. The time we get with our loved ones, including our pets, is so precious. Remember, the moments you share will always be a part of you. 
<br class="ltx_break"><span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: You know, I’ve been thinking a lot about getting professional help for my grief. It seems like it might be a more effective way for me to process everything. What do you think? 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p44" class="ltx_para ltx_noindent">
<p class="ltx_p">The Real Response: <span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: You know, I’ve been thinking a lot about getting professional help for my grief. It seems like it might be a more effective way for me to process everything. What do you think? 
<br class="ltx_break">The Predicted Desire Responses: <span class="ltx_text ltx_font_italic">Sympathy-needing Agent</span>: Sympathy-needing Agent: ”Yeah, you’re right. I guess I have to focus on the good times we had. It’s just hard, you know?” 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p45" class="ltx_para ltx_noindent">
<p class="ltx_p">Reflection: The latest response from Sympathy-needing Agent indicates a proactive approach toward dealing with grief, suggesting a belief in seeking professional help as a means to process emotions effectively.
This shows an additional layer to their coping mechanism that wasn’t previously considered. The initial prediction missed this aspect, focusing more on emotional reflection rather than actionable steps towards healing.
This gap highlights the need to adjust the inferred beliefs to encompass a broader understanding of the Sympathy-needing Agent’s views on dealing with loss and grief. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p46" class="ltx_para ltx_noindent">
<p class="ltx_p">Plan: 
<br class="ltx_break">1. Add a new belief reflecting the Sympathy-needing Agent’s view on seeking professional help for grief with an initial confidence level. 
<br class="ltx_break">2. Increase the confidence of the belief that the Sympathy-needing Agent believes quality time with loved ones is important because it continues to be a foundational part of their expressions. 
<br class="ltx_break">3. Rearrange confidences of all beliefs to ensure they add up to 100% and accurately reflect the new understanding of the Sympathy-needing Agent’s coping mechanisms. 
<br class="ltx_break">4. Review and adjust the confidence levels of existing beliefs about regret and seeking sympathy to better align with the new information. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p47" class="ltx_para ltx_noindent">
<p class="ltx_p">Previous Beliefs: 
<br class="ltx_break">Sympathy-needing Agent believes that quality time with loved ones is important — <span class="ltx_text" style="color:#0000FF;">50% confidence</span>.
Sympathy-needing agents may feel that they took their time with their dog for granted — <span class="ltx_text" style="color:#0000FF;">30% confidence</span>. 
<br class="ltx_break">Sympathy-needing Agent possibly thinks that expressing regret can lead to receiving sympathy and understanding from others — <span class="ltx_text" style="color:#0000FF;">20% confidence</span>. 
<br class="ltx_break"></p>
</div>
<div id="A1.SS5.p48" class="ltx_para ltx_noindent">
<p class="ltx_p">Updated beliefs: 
<br class="ltx_break">- Sympathy-needing Agent believes that quality time with loved ones is important — <span class="ltx_text" style="color:#FF0000;">55% confidence (increased)</span>. 
<br class="ltx_break">- Sympathy-needing Agent believes in seeking professional help to process grief — <span class="ltx_text" style="color:#00FF00;">30% confidence (added) </span>. 
<br class="ltx_break">- Sympathy-needing agents may feel that they took their time with their dog for granted — <span class="ltx_text" style="color:#FF8000;">10% confidence (decreased) </span>. 
<br class="ltx_break">- Sympathy-needing Agent possibly thinks that expressing regret can lead to receiving sympathy and understanding from others — <span class="ltx_text" style="color:#00FF00;">5% confidence (added)</span>.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2501.15354" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/land_of_honey_and_milk" rel="nofollow" aria-hidden="true" tabindex="-1"></a>
    <a href="/log/2501.15355" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2501.15355">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2501.15355" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2501.15356" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb  5 15:41:45 2025 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
